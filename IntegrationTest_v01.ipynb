{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:43 DEBUG gensim.models.doc2vec: Fast version of gensim.models.doc2vec is being used\n",
      "26-Jan-2017 17:17:43 INFO summa.preprocessing.cleaner: 'pattern' package not found; tag filters are not available for English\n",
      "26-Jan-2017 17:17:44 INFO root: Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt\n",
      "26-Jan-2017 17:17:44 INFO root: Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n",
      "/home/tv/.virtualenvs/Topics/lib/python3.5/site-packages/funcy/decorators.py:56: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  spec = inspect.getargspec(func)\n"
     ]
    }
   ],
   "source": [
    "from dariah_topics import preprocessing as pre\n",
    "from dariah_topics import visualization as visual\n",
    "from dariah_topics import mallet as mal\n",
    "# Warning is Gensim related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liste mit Dateinamen erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:44 INFO preprocessing: Creating document list from TXT files ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: 17 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_txt/Doyle_AScandalinBohemia.txt',\n",
       " 'corpus_txt/Doyle_AStudyinScarlet.txt',\n",
       " 'corpus_txt/Doyle_TheHoundoftheBaskervilles.txt',\n",
       " 'corpus_txt/Doyle_TheSignoftheFour.txt',\n",
       " 'corpus_txt/Howard_GodsoftheNorth.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_txt = \"corpus_txt\"\n",
    "#path_txt = \"grenzbote_plain/*/\"\n",
    "\n",
    "doclist_txt = pre.create_document_list(path_txt)\n",
    "assert doclist_txt, \"No documents found\"\n",
    "doclist_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:44 INFO preprocessing: Creating document list from CSV files ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: 16 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_csv/Doyle_AStudyinScarlet.txt.csv',\n",
       " 'corpus_csv/Doyle_TheHoundoftheBaskervilles.txt.csv',\n",
       " 'corpus_csv/Doyle_TheSignoftheFour.txt.csv',\n",
       " 'corpus_csv/Howard_GodsoftheNorth.txt.csv',\n",
       " 'corpus_csv/Howard_SchadowsinZamboula.txt.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_csv = \"corpus_csv\"\n",
    "\n",
    "doclist_csv = pre.create_document_list(path_csv, 'csv')\n",
    "doclist_csv[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Liste mit Dokumentenlabels erzeugen - (Funktion wird durch Thorsten's generischere Funktion ersetzt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:44 INFO preprocessing: Creating document labels ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Document labels available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_txt/Doyle_AScandalinBohemia.txt',\n",
       " 'corpus_txt/Doyle_AStudyinScarlet.txt',\n",
       " 'corpus_txt/Doyle_TheHoundoftheBaskervilles.txt',\n",
       " 'corpus_txt/Doyle_TheSignoftheFour.txt',\n",
       " 'corpus_txt/Howard_GodsoftheNorth.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_labels = list(pre.get_labels(doclist_txt))\n",
    "doc_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_txt = pre.read_from_txt(doclist_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_csv = pre.read_from_csv(doclist_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_AScandalinBohemia.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_AStudyinScarlet.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_TheHoundoftheBaskervilles.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Doyle_TheSignoftheFour.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_GodsoftheNorth.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_SchadowsinZamboula.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_ShadowsintheMoonlight.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Howard_TheDevilinIron.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_TheEndofthePassage.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_TheJungleBook.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Kipling_ThyServantaDog.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Lovecraft_AttheMountainofMadness.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Lovecraft_TheShunnedHouse.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_EurekaAProsePoem.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_TheCaskofAmontillado.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_TheMasqueoftheRedDeath.txt ...\n",
      "26-Jan-2017 17:17:44 DEBUG preprocessing: Accessing TXT document corpus_txt/Poe_ThePurloinedLetter.txt ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a',\n",
       "  'scandal',\n",
       "  'in',\n",
       "  'bohemia',\n",
       "  'a',\n",
       "  'conan',\n",
       "  'doyle',\n",
       "  'i',\n",
       "  'to',\n",
       "  'sherlock',\n",
       "  'holmes',\n",
       "  'she',\n",
       "  'is',\n",
       "  'always',\n",
       "  'the',\n",
       "  'woman',\n",
       "  'i',\n",
       "  'have',\n",
       "  'seldom',\n",
       "  'heard',\n",
       "  'him',\n",
       "  'mention',\n",
       "  'her',\n",
       "  'under',\n",
       "  'any',\n",
       "  'other',\n",
       "  'name',\n",
       "  'in',\n",
       "  'his',\n",
       "  'eyes',\n",
       "  'she',\n",
       "  'eclipses',\n",
       "  'and',\n",
       "  'predominates',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'of',\n",
       "  'her',\n",
       "  'sex',\n",
       "  'it',\n",
       "  'was',\n",
       "  'not',\n",
       "  'that',\n",
       "  'he',\n",
       "  'felt',\n",
       "  'any',\n",
       "  'emotion',\n",
       "  'akin',\n",
       "  'to',\n",
       "  'love',\n",
       "  'for',\n",
       "  'irene',\n",
       "  'adler',\n",
       "  'all',\n",
       "  'emotions',\n",
       "  'and',\n",
       "  'that',\n",
       "  'one',\n",
       "  'particularly',\n",
       "  'were',\n",
       "  'abhorrent',\n",
       "  'to',\n",
       "  'his',\n",
       "  'cold',\n",
       "  'precise',\n",
       "  'but',\n",
       "  'admirably',\n",
       "  'balanced',\n",
       "  'mind',\n",
       "  'he',\n",
       "  'was',\n",
       "  'i',\n",
       "  'take',\n",
       "  'it',\n",
       "  'the',\n",
       "  'most',\n",
       "  'perfect',\n",
       "  'reasoning',\n",
       "  'and',\n",
       "  'observing',\n",
       "  'machine',\n",
       "  'that',\n",
       "  'the',\n",
       "  'world',\n",
       "  'has',\n",
       "  'seen',\n",
       "  'but',\n",
       "  'as',\n",
       "  'a',\n",
       "  'lover',\n",
       "  'he',\n",
       "  'would',\n",
       "  'have',\n",
       "  'placed',\n",
       "  'himself',\n",
       "  'in',\n",
       "  'a',\n",
       "  'false',\n",
       "  'position',\n",
       "  'he',\n",
       "  'never',\n",
       "  'spoke',\n",
       "  'of',\n",
       "  'the',\n",
       "  'softer',\n",
       "  'passions',\n",
       "  'save',\n",
       "  'with',\n",
       "  'a',\n",
       "  'gibe',\n",
       "  'and',\n",
       "  'a',\n",
       "  'sneer',\n",
       "  'they',\n",
       "  'were',\n",
       "  'admirable',\n",
       "  'things',\n",
       "  'for',\n",
       "  'the',\n",
       "  'observer--excellent',\n",
       "  'for',\n",
       "  'drawing',\n",
       "  'the',\n",
       "  'veil',\n",
       "  'from',\n",
       "  \"men's\",\n",
       "  'motives',\n",
       "  'and',\n",
       "  'actions',\n",
       "  'but',\n",
       "  'for',\n",
       "  'the',\n",
       "  'trained',\n",
       "  'reasoner',\n",
       "  'to',\n",
       "  'admit',\n",
       "  'such',\n",
       "  'intrusions',\n",
       "  'into',\n",
       "  'his',\n",
       "  'own',\n",
       "  'delicate',\n",
       "  'and',\n",
       "  'finely',\n",
       "  'adjusted',\n",
       "  'temperament',\n",
       "  'was',\n",
       "  'to',\n",
       "  'introduce',\n",
       "  'a',\n",
       "  'distracting',\n",
       "  'factor',\n",
       "  'which',\n",
       "  'might',\n",
       "  'throw',\n",
       "  'a',\n",
       "  'doubt',\n",
       "  'upon',\n",
       "  'all',\n",
       "  'his',\n",
       "  'mental',\n",
       "  'results',\n",
       "  'grit',\n",
       "  'in',\n",
       "  'a',\n",
       "  'sensitive',\n",
       "  'instrument',\n",
       "  'or',\n",
       "  'a',\n",
       "  'crack',\n",
       "  'in',\n",
       "  'one',\n",
       "  'of',\n",
       "  'his',\n",
       "  'own',\n",
       "  'high-power',\n",
       "  'lenses',\n",
       "  'would',\n",
       "  'not',\n",
       "  'be',\n",
       "  'more',\n",
       "  'disturbing',\n",
       "  'than',\n",
       "  'a',\n",
       "  'strong',\n",
       "  'emotion',\n",
       "  'in',\n",
       "  'a',\n",
       "  'nature',\n",
       "  'such',\n",
       "  'as',\n",
       "  'his',\n",
       "  'and',\n",
       "  'yet',\n",
       "  'there',\n",
       "  'was',\n",
       "  'but',\n",
       "  'one',\n",
       "  'woman',\n",
       "  'to',\n",
       "  'him',\n",
       "  'and',\n",
       "  'that',\n",
       "  'woman',\n",
       "  'was',\n",
       "  'the',\n",
       "  'late',\n",
       "  'irene',\n",
       "  'adler',\n",
       "  'of',\n",
       "  'dubious',\n",
       "  'and',\n",
       "  'questionable',\n",
       "  'memory',\n",
       "  'i',\n",
       "  'had',\n",
       "  'seen',\n",
       "  'little',\n",
       "  'of',\n",
       "  'holmes',\n",
       "  'lately',\n",
       "  'my',\n",
       "  'marriage',\n",
       "  'had',\n",
       "  'drifted',\n",
       "  'us',\n",
       "  'away',\n",
       "  'from',\n",
       "  'each',\n",
       "  'other',\n",
       "  'my',\n",
       "  'own',\n",
       "  'complete',\n",
       "  'happiness',\n",
       "  'and',\n",
       "  'the',\n",
       "  'home-centred',\n",
       "  'interests',\n",
       "  'which',\n",
       "  'rise',\n",
       "  'up',\n",
       "  'around',\n",
       "  'the',\n",
       "  'man',\n",
       "  'who',\n",
       "  'first',\n",
       "  'finds',\n",
       "  'himself',\n",
       "  'master',\n",
       "  'of',\n",
       "  'his',\n",
       "  'own',\n",
       "  'establishment',\n",
       "  'were',\n",
       "  'sufficient',\n",
       "  'to',\n",
       "  'absorb',\n",
       "  'all',\n",
       "  'my',\n",
       "  'attention',\n",
       "  'while',\n",
       "  'holmes',\n",
       "  'who',\n",
       "  'loathed',\n",
       "  'every',\n",
       "  'form',\n",
       "  'of',\n",
       "  'society',\n",
       "  'with',\n",
       "  'his',\n",
       "  'whole',\n",
       "  'bohemian',\n",
       "  'soul',\n",
       "  'remained',\n",
       "  'in',\n",
       "  'our',\n",
       "  'lodgings',\n",
       "  'in',\n",
       "  'baker',\n",
       "  'street',\n",
       "  'buried',\n",
       "  'among',\n",
       "  'his',\n",
       "  'old',\n",
       "  'books',\n",
       "  'and',\n",
       "  'alternating',\n",
       "  'from',\n",
       "  'week',\n",
       "  'to',\n",
       "  'week',\n",
       "  'between',\n",
       "  'cocaine',\n",
       "  'and',\n",
       "  'ambition',\n",
       "  'the',\n",
       "  'drowsiness',\n",
       "  'of',\n",
       "  'the',\n",
       "  'drug',\n",
       "  'and',\n",
       "  'the',\n",
       "  'fierce',\n",
       "  'energy',\n",
       "  'of',\n",
       "  'his',\n",
       "  'own',\n",
       "  'keen',\n",
       "  'nature',\n",
       "  'he',\n",
       "  'was',\n",
       "  'still',\n",
       "  'as',\n",
       "  'ever',\n",
       "  'deeply',\n",
       "  'attracted',\n",
       "  'by',\n",
       "  'the',\n",
       "  'study',\n",
       "  'of',\n",
       "  'crime',\n",
       "  'and',\n",
       "  'occupied',\n",
       "  'his',\n",
       "  'immense',\n",
       "  'faculties',\n",
       "  'and',\n",
       "  'extraordinary',\n",
       "  'powers',\n",
       "  'of',\n",
       "  'observation',\n",
       "  'in',\n",
       "  'following',\n",
       "  'out',\n",
       "  'those',\n",
       "  'clews',\n",
       "  'and',\n",
       "  'clearing',\n",
       "  'up',\n",
       "  'those',\n",
       "  'mysteries',\n",
       "  'which',\n",
       "  'had',\n",
       "  'been',\n",
       "  'abandoned',\n",
       "  'as',\n",
       "  'hopeless',\n",
       "  'by',\n",
       "  'the',\n",
       "  'official',\n",
       "  'police',\n",
       "  'from',\n",
       "  'time',\n",
       "  'to',\n",
       "  'time',\n",
       "  'i',\n",
       "  'heard',\n",
       "  'some',\n",
       "  'vague',\n",
       "  'account',\n",
       "  'of',\n",
       "  'his',\n",
       "  'doings',\n",
       "  'of',\n",
       "  'his',\n",
       "  'summons',\n",
       "  'to',\n",
       "  'odessa',\n",
       "  'in',\n",
       "  'the',\n",
       "  'case',\n",
       "  'of',\n",
       "  'the',\n",
       "  'trepoff',\n",
       "  'murder',\n",
       "  'of',\n",
       "  'his',\n",
       "  'clearing',\n",
       "  'up',\n",
       "  'of',\n",
       "  'the',\n",
       "  'singular',\n",
       "  'tragedy',\n",
       "  'of',\n",
       "  'the',\n",
       "  'atkinson',\n",
       "  'brothers',\n",
       "  'at',\n",
       "  'trincomalee',\n",
       "  'and',\n",
       "  'finally',\n",
       "  'of',\n",
       "  'the',\n",
       "  'mission',\n",
       "  'which',\n",
       "  'he',\n",
       "  'had',\n",
       "  'accomplished',\n",
       "  'so',\n",
       "  'delicately',\n",
       "  'and',\n",
       "  'successfully',\n",
       "  'for',\n",
       "  'the',\n",
       "  'reigning',\n",
       "  'family',\n",
       "  'of',\n",
       "  'holland',\n",
       "  'beyond',\n",
       "  'these',\n",
       "  'signs',\n",
       "  'of',\n",
       "  'his',\n",
       "  'activity',\n",
       "  'however',\n",
       "  'which',\n",
       "  'i',\n",
       "  'merely',\n",
       "  'shared',\n",
       "  'with',\n",
       "  'all',\n",
       "  'the',\n",
       "  'readers',\n",
       "  'of',\n",
       "  'the',\n",
       "  'daily',\n",
       "  'press',\n",
       "  'i',\n",
       "  'knew',\n",
       "  'little',\n",
       "  'of',\n",
       "  'my',\n",
       "  'former',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'companion',\n",
       "  'one',\n",
       "  'night--it',\n",
       "  'was',\n",
       "  'on',\n",
       "  'the',\n",
       "  'th',\n",
       "  'of',\n",
       "  'march',\n",
       "  'i',\n",
       "  'was',\n",
       "  'returning',\n",
       "  'from',\n",
       "  'a',\n",
       "  'journey',\n",
       "  'to',\n",
       "  'a',\n",
       "  'patient',\n",
       "  'for',\n",
       "  'i',\n",
       "  'had',\n",
       "  'now',\n",
       "  'returned',\n",
       "  'to',\n",
       "  'civil',\n",
       "  'practice',\n",
       "  'when',\n",
       "  'my',\n",
       "  'way',\n",
       "  'led',\n",
       "  'me',\n",
       "  'through',\n",
       "  'baker',\n",
       "  'street',\n",
       "  'as',\n",
       "  'i',\n",
       "  'passed',\n",
       "  'the',\n",
       "  'well-remembered',\n",
       "  'door',\n",
       "  'which',\n",
       "  'must',\n",
       "  'always',\n",
       "  'be',\n",
       "  'associated',\n",
       "  'in',\n",
       "  'my',\n",
       "  'mind',\n",
       "  'with',\n",
       "  'my',\n",
       "  'wooing',\n",
       "  'and',\n",
       "  'with',\n",
       "  'the',\n",
       "  'dark',\n",
       "  'incidents',\n",
       "  'of',\n",
       "  'the',\n",
       "  'study',\n",
       "  'in',\n",
       "  'scarlet',\n",
       "  'i',\n",
       "  'was',\n",
       "  'seized',\n",
       "  'with',\n",
       "  'a',\n",
       "  'keen',\n",
       "  'desire',\n",
       "  'to',\n",
       "  'see',\n",
       "  'holmes',\n",
       "  'again',\n",
       "  'and',\n",
       "  'to',\n",
       "  'know',\n",
       "  'how',\n",
       "  'he',\n",
       "  'was',\n",
       "  'employing',\n",
       "  'his',\n",
       "  'extraordinary',\n",
       "  'powers',\n",
       "  'his',\n",
       "  'rooms',\n",
       "  'were',\n",
       "  'brilliantly',\n",
       "  'lighted',\n",
       "  'and',\n",
       "  'even',\n",
       "  'as',\n",
       "  'i',\n",
       "  'looked',\n",
       "  'up',\n",
       "  'i',\n",
       "  'saw',\n",
       "  'his',\n",
       "  'tall',\n",
       "  'spare',\n",
       "  'figure',\n",
       "  'pass',\n",
       "  'twice',\n",
       "  'in',\n",
       "  'a',\n",
       "  'dark',\n",
       "  'silhouette',\n",
       "  'against',\n",
       "  'the',\n",
       "  'blind',\n",
       "  'he',\n",
       "  'was',\n",
       "  'pacing',\n",
       "  'the',\n",
       "  'room',\n",
       "  'swiftly',\n",
       "  'eagerly',\n",
       "  'with',\n",
       "  'his',\n",
       "  'head',\n",
       "  'sunk',\n",
       "  'upon',\n",
       "  'his',\n",
       "  'chest',\n",
       "  'and',\n",
       "  'his',\n",
       "  'hands',\n",
       "  'clasped',\n",
       "  'behind',\n",
       "  'him',\n",
       "  'to',\n",
       "  'me',\n",
       "  'who',\n",
       "  'knew',\n",
       "  'his',\n",
       "  'every',\n",
       "  'mood',\n",
       "  'and',\n",
       "  'habit',\n",
       "  'his',\n",
       "  'attitude',\n",
       "  'and',\n",
       "  'manner',\n",
       "  'told',\n",
       "  'their',\n",
       "  'own',\n",
       "  'story',\n",
       "  'he',\n",
       "  'was',\n",
       "  'at',\n",
       "  'work',\n",
       "  'again',\n",
       "  'he',\n",
       "  'had',\n",
       "  'risen',\n",
       "  'out',\n",
       "  'of',\n",
       "  'his',\n",
       "  'drug-created',\n",
       "  'dreams',\n",
       "  'and',\n",
       "  'was',\n",
       "  'hot',\n",
       "  'upon',\n",
       "  'the',\n",
       "  'scent',\n",
       "  'of',\n",
       "  'some',\n",
       "  'new',\n",
       "  'problem',\n",
       "  'i',\n",
       "  'rang',\n",
       "  'the',\n",
       "  'bell',\n",
       "  'and',\n",
       "  'was',\n",
       "  'shown',\n",
       "  'up',\n",
       "  'to',\n",
       "  'the',\n",
       "  'chamber',\n",
       "  'which',\n",
       "  'had',\n",
       "  'formerly',\n",
       "  'been',\n",
       "  'in',\n",
       "  'part',\n",
       "  'my',\n",
       "  'own',\n",
       "  'his',\n",
       "  'manner',\n",
       "  'was',\n",
       "  'not',\n",
       "  'effusive',\n",
       "  'it',\n",
       "  'seldom',\n",
       "  'was',\n",
       "  'but',\n",
       "  'he',\n",
       "  'was',\n",
       "  'glad',\n",
       "  'i',\n",
       "  'think',\n",
       "  'to',\n",
       "  'see',\n",
       "  'me',\n",
       "  'with',\n",
       "  'hardly',\n",
       "  'a',\n",
       "  'word',\n",
       "  'spoken',\n",
       "  'but',\n",
       "  'with',\n",
       "  'a',\n",
       "  'kindly',\n",
       "  'eye',\n",
       "  'he',\n",
       "  'waved',\n",
       "  'me',\n",
       "  'to',\n",
       "  'an',\n",
       "  'armchair',\n",
       "  'threw',\n",
       "  'across',\n",
       "  'his',\n",
       "  'case',\n",
       "  'of',\n",
       "  'cigars',\n",
       "  'and',\n",
       "  'indicated',\n",
       "  'a',\n",
       "  'spirit',\n",
       "  'case',\n",
       "  'and',\n",
       "  'a',\n",
       "  'gasogene',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corner',\n",
       "  'then',\n",
       "  'he',\n",
       "  'stood',\n",
       "  'before',\n",
       "  'the',\n",
       "  'fire',\n",
       "  'and',\n",
       "  'looked',\n",
       "  'me',\n",
       "  'over',\n",
       "  'in',\n",
       "  'his',\n",
       "  'singular',\n",
       "  'introspective',\n",
       "  'fashion',\n",
       "  'wedlock',\n",
       "  'suits',\n",
       "  'you',\n",
       "  'he',\n",
       "  'remarked',\n",
       "  'i',\n",
       "  'think',\n",
       "  'watson',\n",
       "  'that',\n",
       "  'you',\n",
       "  'have',\n",
       "  'put',\n",
       "  'on',\n",
       "  'seven',\n",
       "  'and',\n",
       "  'a',\n",
       "  'half',\n",
       "  'pounds',\n",
       "  'since',\n",
       "  'i',\n",
       "  'saw',\n",
       "  'you',\n",
       "  'seven',\n",
       "  'i',\n",
       "  'answered',\n",
       "  'indeed',\n",
       "  'i',\n",
       "  'should',\n",
       "  'have',\n",
       "  'thought',\n",
       "  'a',\n",
       "  'little',\n",
       "  'more',\n",
       "  'just',\n",
       "  'a',\n",
       "  'trifle',\n",
       "  'more',\n",
       "  'i',\n",
       "  'fancy',\n",
       "  'watson',\n",
       "  'and',\n",
       "  'in',\n",
       "  'practice',\n",
       "  'again',\n",
       "  'i',\n",
       "  'observe',\n",
       "  'you',\n",
       "  'did',\n",
       "  'not',\n",
       "  'tell',\n",
       "  'me',\n",
       "  'that',\n",
       "  'you',\n",
       "  'intended',\n",
       "  'to',\n",
       "  'go',\n",
       "  'into',\n",
       "  'harness',\n",
       "  'then',\n",
       "  'how',\n",
       "  'do',\n",
       "  'you',\n",
       "  'know',\n",
       "  'i',\n",
       "  'see',\n",
       "  'it',\n",
       "  'i',\n",
       "  'deduce',\n",
       "  'it',\n",
       "  'how',\n",
       "  'do',\n",
       "  'i',\n",
       "  'know',\n",
       "  'that',\n",
       "  'you',\n",
       "  'have',\n",
       "  'been',\n",
       "  'getting',\n",
       "  'yourself',\n",
       "  'very',\n",
       "  'wet',\n",
       "  'lately',\n",
       "  'and',\n",
       "  'that',\n",
       "  'you',\n",
       "  'have',\n",
       "  'a',\n",
       "  'most',\n",
       "  'clumsy',\n",
       "  'and',\n",
       "  'careless',\n",
       "  'servant',\n",
       "  'girl',\n",
       "  'my',\n",
       "  'dear',\n",
       "  'holmes',\n",
       "  'said',\n",
       "  'i',\n",
       "  'this',\n",
       "  'is',\n",
       "  'too',\n",
       "  'much',\n",
       "  'you',\n",
       "  'would',\n",
       "  'certainly',\n",
       "  'have',\n",
       "  'been',\n",
       "  'burned',\n",
       "  'had',\n",
       "  'you',\n",
       "  'lived',\n",
       "  'a',\n",
       "  'few',\n",
       "  'centuries',\n",
       "  'ago',\n",
       "  'it',\n",
       "  'is',\n",
       "  'true',\n",
       "  'that',\n",
       "  'i',\n",
       "  'had',\n",
       "  'a',\n",
       "  'country',\n",
       "  'walk',\n",
       "  'on',\n",
       "  'thursday',\n",
       "  'and',\n",
       "  'came',\n",
       "  'home',\n",
       "  'in',\n",
       "  'a',\n",
       "  'dreadful',\n",
       "  'mess',\n",
       "  'but',\n",
       "  'as',\n",
       "  'i',\n",
       "  'have',\n",
       "  'changed',\n",
       "  'my',\n",
       "  'clothes',\n",
       "  'i',\n",
       "  \"can't\",\n",
       "  'imagine',\n",
       "  'how',\n",
       "  'you',\n",
       "  'deduce',\n",
       "  'it',\n",
       "  'as',\n",
       "  'to',\n",
       "  'mary',\n",
       "  'jane',\n",
       "  'she',\n",
       "  'is',\n",
       "  'incorrigible',\n",
       "  'and',\n",
       "  'my',\n",
       "  'wife',\n",
       "  'has',\n",
       "  'given',\n",
       "  'her',\n",
       "  'notice',\n",
       "  'but',\n",
       "  'there',\n",
       "  'again',\n",
       "  'i',\n",
       "  'fail',\n",
       "  'to',\n",
       "  'see',\n",
       "  'how',\n",
       "  'you',\n",
       "  'work',\n",
       "  'it',\n",
       "  'out',\n",
       "  'he',\n",
       "  'chuckled',\n",
       "  'to',\n",
       "  'himself',\n",
       "  'and',\n",
       "  'rubbed',\n",
       "  'his',\n",
       "  'long',\n",
       "  'nervous',\n",
       "  'hands',\n",
       "  'together',\n",
       "  'it',\n",
       "  'is',\n",
       "  'simplicity',\n",
       "  'itself',\n",
       "  'said',\n",
       "  'he',\n",
       "  'my',\n",
       "  'eyes',\n",
       "  'tell',\n",
       "  'me',\n",
       "  'that',\n",
       "  'on',\n",
       "  'the',\n",
       "  'inside',\n",
       "  'of',\n",
       "  'your',\n",
       "  'left',\n",
       "  'shoe',\n",
       "  'just',\n",
       "  'where',\n",
       "  'the',\n",
       "  'firelight',\n",
       "  'strikes',\n",
       "  'it',\n",
       "  'the',\n",
       "  'leather',\n",
       "  'is',\n",
       "  'scored',\n",
       "  'by',\n",
       "  'six',\n",
       "  'almost',\n",
       "  'parallel',\n",
       "  'cuts',\n",
       "  'obviously',\n",
       "  'they',\n",
       "  'have',\n",
       "  'been',\n",
       "  'caused',\n",
       "  'by',\n",
       "  'someone',\n",
       "  'who',\n",
       "  'has',\n",
       "  'very',\n",
       "  'carelessly',\n",
       "  'scraped',\n",
       "  'round',\n",
       "  'the',\n",
       "  'edges',\n",
       "  'of',\n",
       "  'the',\n",
       "  'sole',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'remove',\n",
       "  'crusted',\n",
       "  'mud',\n",
       "  'from',\n",
       "  'it',\n",
       "  'hence',\n",
       "  'you',\n",
       "  'see',\n",
       "  'my',\n",
       "  'double',\n",
       "  'deduction',\n",
       "  'that',\n",
       "  'you',\n",
       "  'had',\n",
       "  'been',\n",
       "  'out',\n",
       "  'in',\n",
       "  'vile',\n",
       "  'weather',\n",
       "  'and',\n",
       "  'that',\n",
       "  'you',\n",
       "  'had',\n",
       "  'a',\n",
       "  'particularly',\n",
       "  'malignant',\n",
       "  'boot-slicking',\n",
       "  'specimen',\n",
       "  'of',\n",
       "  'the',\n",
       "  'london',\n",
       "  'slavey',\n",
       "  'as',\n",
       "  'to',\n",
       "  'your',\n",
       "  'practice',\n",
       "  'if',\n",
       "  'a',\n",
       "  'gentleman',\n",
       "  'walks',\n",
       "  'into',\n",
       "  'my',\n",
       "  'rooms',\n",
       "  'smelling',\n",
       "  'of',\n",
       "  'iodoform',\n",
       "  'with',\n",
       "  'a',\n",
       "  'black',\n",
       "  'mark',\n",
       "  'of',\n",
       "  'nitrate',\n",
       "  'of',\n",
       "  'silver',\n",
       "  'upon',\n",
       "  'his',\n",
       "  'right',\n",
       "  'forefinger',\n",
       "  'and',\n",
       "  'a',\n",
       "  'bulge',\n",
       "  'on',\n",
       "  'the',\n",
       "  'side',\n",
       "  'of',\n",
       "  'his',\n",
       "  'top',\n",
       "  'hat',\n",
       "  'to',\n",
       "  'show',\n",
       "  'where',\n",
       "  'he',\n",
       "  'has',\n",
       "  'secreted',\n",
       "  'his',\n",
       "  ...]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = [list(pre.tokenize(txt)) for txt in list(corpus_txt)]\n",
    "doc_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_types, doc_ids = pre.create_dictionaries(doc_labels, doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sparse_bow = pre.create_mm(doc_labels, doc_tokens, id_types, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23894</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16390</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6145</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20488</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10639</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15705</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20504</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16409</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10927</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16415</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17756</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16426</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12295</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13095</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15027</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20532</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20535</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12345</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">16</th>\n",
       "      <th>6096</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8146</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14292</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24537</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16346</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9551</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8156</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24542</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4065</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12206</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12265</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18411</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10220</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17064</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10228</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4777</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18430</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56956 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "doc_id token_id    \n",
       "1      3          1\n",
       "       23894      1\n",
       "       16390      1\n",
       "       6145       4\n",
       "       20488      2\n",
       "       9          1\n",
       "       8202       1\n",
       "       16398      1\n",
       "       8207       3\n",
       "       10639      1\n",
       "       15705      3\n",
       "       20504      1\n",
       "       16409      1\n",
       "       10927      2\n",
       "       4125       2\n",
       "       30         2\n",
       "       16415     17\n",
       "       38         9\n",
       "       689        1\n",
       "       17756      1\n",
       "       16426      4\n",
       "       12295      1\n",
       "       4141       1\n",
       "       8239      16\n",
       "       13095      1\n",
       "       15027      1\n",
       "       20532      2\n",
       "       14059      1\n",
       "       20535      2\n",
       "       12345     56\n",
       "...              ..\n",
       "16     6096       0\n",
       "       11256      0\n",
       "       8146       0\n",
       "       14292      0\n",
       "       10197      0\n",
       "       10199      0\n",
       "       24537      0\n",
       "       16346      0\n",
       "       9551       0\n",
       "       8156       0\n",
       "       24542      0\n",
       "       22496      0\n",
       "       4065       0\n",
       "       22499      0\n",
       "       8164       0\n",
       "       12206      0\n",
       "       14312      0\n",
       "       12265      0\n",
       "       18411      0\n",
       "       10220      0\n",
       "       17064      0\n",
       "       2034       0\n",
       "       22515      0\n",
       "       10228      0\n",
       "       886        0\n",
       "       10230      0\n",
       "       4777       0\n",
       "       2044       0\n",
       "       18430      0\n",
       "       6143       0\n",
       "\n",
       "[56956 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Doc-Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\ndoc2id = {value : key for key, value in doc_ids.items()}\\ntype2id = {value : key for key, value in id_types.items()}\\n\\ncols = [doc2id[key] for key in set(sparse_bow.index.get_level_values(\"doc_id\"))]\\n#idx = [type2id[key] for key in set(sparse_bow.index.get_level_values(\"token_id\"))]\\n\\nset(sparse_bow.index.get_level_values(\"token_id\"))\\n\\n#doctopic_matrix = pd.DataFrame(columns=cols, index=idx)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "doc2id = {value : key for key, value in doc_ids.items()}\n",
    "type2id = {value : key for key, value in id_types.items()}\n",
    "\n",
    "cols = [doc2id[key] for key in set(sparse_bow.index.get_level_values(\"doc_id\"))]\n",
    "#idx = [type2id[key] for key in set(sparse_bow.index.get_level_values(\"token_id\"))]\n",
    "\n",
    "set(sparse_bow.index.get_level_values(\"token_id\"))\n",
    "\n",
    "#doctopic_matrix = pd.DataFrame(columns=cols, index=idx)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "basepath =  os.path.abspath(os.path.join(\".\", os.pardir))\n",
    "\n",
    "with open(os.path.join(basepath, \"Topics/tutorial_supplementals/stopwords/en\"), 'r', encoding = 'utf-8') as f: \n",
    "    stopword_list = f.read().split('\\n')\n",
    "    \n",
    "stopword_list = set(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:45 INFO preprocessing: Removing features ...\n",
      "26-Jan-2017 17:17:45 DEBUG preprocessing: 672 features removed.\n"
     ]
    }
   ],
   "source": [
    "sparse_df_stopwords_removed = pre.remove_features(sparse_bow, id_types, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56956\n",
      "51572\n"
     ]
    }
   ],
   "source": [
    "print(len(sparse_bow))\n",
    "print(len(sparse_df_stopwords_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Feature Remove Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:45 INFO preprocessing: Finding stopwords ...\n",
      "26-Jan-2017 17:17:45 DEBUG preprocessing: 100 stopwords found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list = pre.find_stopwords(sparse_bow, id_types, 100)\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:45 INFO preprocessing: Find hapax legomena ...\n",
      "26-Jan-2017 17:17:45 DEBUG preprocessing: 20647 hapax legomena found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20647"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapax_list = pre.find_hapax(sparse_bow, id_types)\n",
    "len(hapax_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:45 INFO preprocessing: Removing features ...\n",
      "26-Jan-2017 17:17:45 DEBUG preprocessing: 20727 features removed.\n"
     ]
    }
   ],
   "source": [
    "feature_list = set(stopword_list).union(hapax_list)\n",
    "clean_term_frequency = pre.remove_features(sparse_bow, id_types, feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5595"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Sparse BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_docs = max(sparse_bow.index.get_level_values(\"doc_id\"))\n",
    "num_types = max(sparse_bow.index.get_level_values(\"token_id\"))\n",
    "sum_counts = sum(sparse_bow[0])\n",
    "\n",
    "header_string = str(num_docs) + \" \" + str(num_types) + \" \" + str(sum_counts) + \"\\n\"\n",
    "\n",
    "with open(\"gb_plain.mm\", 'w', encoding = \"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "with open(\"gb_plain.mm\", 'a', encoding = \"utf-8\") as f:\n",
    "    f.write(\"%%MatrixMarket matrix coordinate real general\\n\")\n",
    "    f.write(header_string)\n",
    "    sparse_bow.to_csv( f, sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:45 INFO gensim.matutils: initializing corpus reader from gb_plain.mm\n",
      "26-Jan-2017 17:17:45 INFO gensim.matutils: accepted corpus with 16 documents, 24552 features, 372528 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "mm = MmCorpus(\"gb_plain.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in case you're only loading the corpus - build dict first\n",
    "\n",
    "#import pickle\n",
    "\n",
    "#pickle.dump( id_types, open( \"gb_plain.dictionary\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#id_types = pickle.load(open(\"gb_plain.dictionary\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mm = gensim.corpora.MmCorpus(\"gb_all.mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#id_types = pickle.load(open(\"gb_all.dictionary\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2id = {value : key for key, value in doc_ids.items()}\n",
    "type2id = {value : key for key, value in id_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:17:46 INFO gensim.models.ldamodel: using symmetric alpha at 0.01\n",
      "26-Jan-2017 17:17:46 INFO gensim.models.ldamodel: using symmetric eta at 4.0728220584042684e-05\n",
      "26-Jan-2017 17:17:46 INFO gensim.models.ldamodel: using serial LDA version on this node\n",
      "26-Jan-2017 17:18:11 INFO gensim.models.ldamodel: running online LDA training, 100 topics, 1 passes over the supplied corpus of 16 documents, updating model once every 16 documents, evaluating perplexity every 16 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "26-Jan-2017 17:18:11 WARNING gensim.models.ldamodel: too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "26-Jan-2017 17:18:37 DEBUG gensim.models.ldamodel: bound: at document #0\n",
      "26-Jan-2017 17:18:41 INFO gensim.models.ldamodel: -31.033 per-word bound, 2196586158.1 perplexity estimate based on a held-out corpus of 16 documents with 372528 words\n",
      "26-Jan-2017 17:18:41 INFO gensim.models.ldamodel: PROGRESS: pass 0, at document #16/16\n",
      "26-Jan-2017 17:18:41 DEBUG gensim.models.ldamodel: performing inference on a chunk of 16 documents\n",
      "26-Jan-2017 17:18:41 DEBUG gensim.models.ldamodel: 1/16 documents converged within 50 iterations\n",
      "26-Jan-2017 17:18:42 DEBUG gensim.models.ldamodel: updating topics\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic #5 (0.010): 0.032*\"short-wave\" + 0.031*\"knifing\" + 0.019*\"satyr-countenance\" + 0.017*\"satisfactorily\" + 0.015*\"leicester\" + 0.014*\"orient\" + 0.012*\"last—though\" + 0.012*\"shuddered\" + 0.009*\"masse\" + 0.009*\"stoat\"\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic #95 (0.010): 0.060*\"short-wave\" + 0.031*\"knifing\" + 0.021*\"shuddered\" + 0.021*\"satisfactorily\" + 0.020*\"last—though\" + 0.018*\"satyr-countenance\" + 0.015*\"masse\" + 0.013*\"leicester\" + 0.013*\"orient\" + 0.012*\"stoat\"\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic #54 (0.010): 0.051*\"short-wave\" + 0.039*\"knifing\" + 0.023*\"last—though\" + 0.022*\"satisfactorily\" + 0.020*\"orient\" + 0.018*\"shuddered\" + 0.017*\"satyr-countenance\" + 0.014*\"leicester\" + 0.012*\"bulleana\" + 0.012*\"masse\"\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic #12 (0.010): 0.051*\"short-wave\" + 0.030*\"knifing\" + 0.027*\"last—though\" + 0.027*\"satisfactorily\" + 0.023*\"shuddered\" + 0.019*\"orient\" + 0.013*\"stoat\" + 0.013*\"satyr-countenance\" + 0.011*\"leicester\" + 0.011*\"callers\"\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic #93 (0.010): 0.056*\"short-wave\" + 0.038*\"knifing\" + 0.028*\"satisfactorily\" + 0.022*\"shuddered\" + 0.018*\"orient\" + 0.016*\"last—though\" + 0.015*\"masse\" + 0.013*\"leicester\" + 0.013*\"satyr-countenance\" + 0.012*\"snuff-and-butter\"\n",
      "26-Jan-2017 17:18:42 INFO gensim.models.ldamodel: topic diff=41.685355, rho=1.000000\n"
     ]
    }
   ],
   "source": [
    "#model = LdaModel(corpus=mm, id2word=type2id, num_topics=60, alpha = \"symmetric\", passes = 10) #import momentan in visual \n",
    "# -> da ich mir noch nicht sicher bin, welche Funktionen in das tm_gensim.py sollen\n",
    "model = LdaModel(corpus=mm, id2word=type2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['short-wave',\n",
       " 'knifing',\n",
       " 'satisfactorily',\n",
       " 'last—though',\n",
       " 'orient',\n",
       " 'shuddered',\n",
       " 'masse',\n",
       " 'leicester',\n",
       " 'satyr-countenance',\n",
       " 'stoat']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_nr_x = model.get_topic_terms(10)\n",
    "\n",
    "[type2id[i[0]] for i in topic_nr_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = model.show_topics(num_topics = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(86,\n",
       "  '0.056*\"short-wave\" + 0.025*\"knifing\" + 0.023*\"satisfactorily\" + 0.018*\"last—though\" + 0.017*\"satyr-countenance\" + 0.017*\"shuddered\" + 0.013*\"orient\" + 0.013*\"masse\" + 0.010*\"stoat\" + 0.010*\"leicester\"'),\n",
       " (57,\n",
       "  '0.079*\"short-wave\" + 0.034*\"satisfactorily\" + 0.028*\"knifing\" + 0.018*\"last—though\" + 0.018*\"shuddered\" + 0.017*\"orient\" + 0.017*\"satyr-countenance\" + 0.015*\"leicester\" + 0.012*\"bulleana\" + 0.011*\"stoat\"'),\n",
       " (55,\n",
       "  '0.038*\"short-wave\" + 0.025*\"satisfactorily\" + 0.020*\"knifing\" + 0.019*\"last—though\" + 0.016*\"shuddered\" + 0.013*\"orient\" + 0.012*\"satyr-countenance\" + 0.011*\"masse\" + 0.009*\"stoat\" + 0.009*\"leicester\"'),\n",
       " (89,\n",
       "  '0.042*\"short-wave\" + 0.026*\"satisfactorily\" + 0.023*\"knifing\" + 0.014*\"last—though\" + 0.012*\"shuddered\" + 0.012*\"orient\" + 0.009*\"snuff-and-butter\" + 0.009*\"masse\" + 0.008*\"satyr-countenance\" + 0.008*\"stoat\"'),\n",
       " (94,\n",
       "  '0.044*\"short-wave\" + 0.019*\"satisfactorily\" + 0.017*\"knifing\" + 0.016*\"shuddered\" + 0.012*\"satyr-countenance\" + 0.012*\"last—though\" + 0.011*\"masse\" + 0.009*\"stoat\" + 0.009*\"bulleana\" + 0.008*\"orient\"'),\n",
       " (29,\n",
       "  '0.051*\"short-wave\" + 0.029*\"knifing\" + 0.021*\"last—though\" + 0.020*\"satisfactorily\" + 0.013*\"shuddered\" + 0.012*\"leicester\" + 0.012*\"masse\" + 0.010*\"satyr-countenance\" + 0.010*\"testify\" + 0.009*\"orient\"'),\n",
       " (80,\n",
       "  '0.027*\"short-wave\" + 0.024*\"knifing\" + 0.016*\"last—though\" + 0.015*\"satyr-countenance\" + 0.014*\"satisfactorily\" + 0.012*\"shuddered\" + 0.008*\"snuff-and-butter\" + 0.008*\"orient\" + 0.008*\"bulleana\" + 0.008*\"stoat\"'),\n",
       " (59,\n",
       "  '0.043*\"short-wave\" + 0.033*\"knifing\" + 0.026*\"satyr-countenance\" + 0.024*\"satisfactorily\" + 0.023*\"last—though\" + 0.022*\"shuddered\" + 0.015*\"leicester\" + 0.014*\"stoat\" + 0.013*\"orient\" + 0.011*\"snuff-and-butter\"'),\n",
       " (65,\n",
       "  '0.048*\"short-wave\" + 0.026*\"satisfactorily\" + 0.022*\"knifing\" + 0.022*\"last—though\" + 0.019*\"shuddered\" + 0.018*\"satyr-countenance\" + 0.015*\"stoat\" + 0.014*\"masse\" + 0.013*\"orient\" + 0.012*\"leicester\"'),\n",
       " (32,\n",
       "  '0.065*\"short-wave\" + 0.033*\"satisfactorily\" + 0.029*\"knifing\" + 0.023*\"last—though\" + 0.021*\"shuddered\" + 0.018*\"orient\" + 0.016*\"masse\" + 0.013*\"bulleana\" + 0.012*\"satyr-countenance\" + 0.009*\"stoat\"'),\n",
       " (97,\n",
       "  '0.070*\"short-wave\" + 0.040*\"satisfactorily\" + 0.027*\"knifing\" + 0.021*\"last—though\" + 0.017*\"orient\" + 0.017*\"shuddered\" + 0.013*\"masse\" + 0.012*\"stoat\" + 0.011*\"snuff-and-butter\" + 0.010*\"shrugged\"'),\n",
       " (40,\n",
       "  '0.047*\"short-wave\" + 0.041*\"knifing\" + 0.027*\"satisfactorily\" + 0.022*\"shuddered\" + 0.022*\"last—though\" + 0.017*\"orient\" + 0.016*\"masse\" + 0.014*\"satyr-countenance\" + 0.013*\"leicester\" + 0.011*\"stoat\"'),\n",
       " (41,\n",
       "  '0.065*\"short-wave\" + 0.026*\"knifing\" + 0.026*\"last—though\" + 0.023*\"satyr-countenance\" + 0.022*\"satisfactorily\" + 0.021*\"shuddered\" + 0.017*\"orient\" + 0.014*\"leicester\" + 0.013*\"snuff-and-butter\" + 0.011*\"stoat\"'),\n",
       " (6,\n",
       "  '0.056*\"short-wave\" + 0.036*\"satisfactorily\" + 0.028*\"knifing\" + 0.023*\"last—though\" + 0.021*\"orient\" + 0.019*\"shuddered\" + 0.017*\"masse\" + 0.017*\"satyr-countenance\" + 0.010*\"snuff-and-butter\" + 0.010*\"stoat\"'),\n",
       " (82,\n",
       "  '0.050*\"short-wave\" + 0.026*\"knifing\" + 0.024*\"masse\" + 0.024*\"last—though\" + 0.022*\"satisfactorily\" + 0.017*\"shuddered\" + 0.015*\"leicester\" + 0.015*\"satyr-countenance\" + 0.015*\"stoat\" + 0.013*\"orient\"'),\n",
       " (31,\n",
       "  '0.063*\"short-wave\" + 0.027*\"satisfactorily\" + 0.027*\"last—though\" + 0.023*\"knifing\" + 0.017*\"shuddered\" + 0.016*\"leicester\" + 0.013*\"orient\" + 0.012*\"masse\" + 0.012*\"satyr-countenance\" + 0.010*\"stoat\"'),\n",
       " (95,\n",
       "  '0.060*\"short-wave\" + 0.031*\"knifing\" + 0.021*\"shuddered\" + 0.021*\"satisfactorily\" + 0.020*\"last—though\" + 0.018*\"satyr-countenance\" + 0.015*\"masse\" + 0.013*\"leicester\" + 0.013*\"orient\" + 0.012*\"stoat\"'),\n",
       " (50,\n",
       "  '0.042*\"short-wave\" + 0.029*\"satisfactorily\" + 0.022*\"knifing\" + 0.022*\"last—though\" + 0.022*\"shuddered\" + 0.020*\"satyr-countenance\" + 0.015*\"leicester\" + 0.014*\"orient\" + 0.013*\"masse\" + 0.012*\"stoat\"'),\n",
       " (56,\n",
       "  '0.040*\"short-wave\" + 0.019*\"knifing\" + 0.017*\"shuddered\" + 0.016*\"satisfactorily\" + 0.015*\"satyr-countenance\" + 0.015*\"orient\" + 0.014*\"last—though\" + 0.012*\"leicester\" + 0.010*\"stoat\" + 0.009*\"snuff-and-butter\"'),\n",
       " (64,\n",
       "  '0.057*\"short-wave\" + 0.032*\"knifing\" + 0.021*\"shuddered\" + 0.021*\"satisfactorily\" + 0.019*\"last—though\" + 0.017*\"masse\" + 0.014*\"leicester\" + 0.013*\"stoat\" + 0.013*\"satyr-countenance\" + 0.012*\"orient\"'),\n",
       " (48,\n",
       "  '0.043*\"short-wave\" + 0.026*\"satisfactorily\" + 0.025*\"knifing\" + 0.023*\"last—though\" + 0.018*\"orient\" + 0.016*\"shuddered\" + 0.014*\"satyr-countenance\" + 0.014*\"leicester\" + 0.009*\"snuff-and-butter\" + 0.009*\"masse\"'),\n",
       " (88,\n",
       "  '0.048*\"short-wave\" + 0.026*\"knifing\" + 0.022*\"satyr-countenance\" + 0.020*\"last—though\" + 0.020*\"satisfactorily\" + 0.020*\"shuddered\" + 0.017*\"leicester\" + 0.015*\"masse\" + 0.013*\"orient\" + 0.012*\"bulleana\"'),\n",
       " (93,\n",
       "  '0.056*\"short-wave\" + 0.038*\"knifing\" + 0.028*\"satisfactorily\" + 0.022*\"shuddered\" + 0.018*\"orient\" + 0.016*\"last—though\" + 0.015*\"masse\" + 0.013*\"leicester\" + 0.013*\"satyr-countenance\" + 0.012*\"snuff-and-butter\"'),\n",
       " (58,\n",
       "  '0.042*\"short-wave\" + 0.023*\"shuddered\" + 0.021*\"knifing\" + 0.020*\"last—though\" + 0.017*\"satisfactorily\" + 0.014*\"leicester\" + 0.013*\"orient\" + 0.012*\"satyr-countenance\" + 0.012*\"snuff-and-butter\" + 0.010*\"masse\"'),\n",
       " (7,\n",
       "  '0.058*\"short-wave\" + 0.027*\"knifing\" + 0.021*\"satisfactorily\" + 0.018*\"last—though\" + 0.017*\"satyr-countenance\" + 0.016*\"shuddered\" + 0.015*\"orient\" + 0.011*\"masse\" + 0.011*\"leicester\" + 0.010*\"bulleana\"'),\n",
       " (79,\n",
       "  '0.067*\"short-wave\" + 0.033*\"knifing\" + 0.027*\"shuddered\" + 0.022*\"last—though\" + 0.021*\"satisfactorily\" + 0.017*\"leicester\" + 0.016*\"satyr-countenance\" + 0.014*\"snuff-and-butter\" + 0.012*\"bulleana\" + 0.011*\"masse\"'),\n",
       " (36,\n",
       "  '0.051*\"short-wave\" + 0.033*\"satisfactorily\" + 0.033*\"knifing\" + 0.022*\"last—though\" + 0.021*\"shuddered\" + 0.019*\"satyr-countenance\" + 0.018*\"orient\" + 0.016*\"leicester\" + 0.013*\"snuff-and-butter\" + 0.011*\"masse\"'),\n",
       " (46,\n",
       "  '0.053*\"short-wave\" + 0.040*\"satisfactorily\" + 0.028*\"knifing\" + 0.019*\"last—though\" + 0.019*\"satyr-countenance\" + 0.017*\"shuddered\" + 0.017*\"leicester\" + 0.014*\"orient\" + 0.012*\"masse\" + 0.012*\"snuff-and-butter\"'),\n",
       " (19,\n",
       "  '0.064*\"short-wave\" + 0.039*\"satisfactorily\" + 0.026*\"knifing\" + 0.020*\"orient\" + 0.020*\"last—though\" + 0.019*\"shuddered\" + 0.015*\"masse\" + 0.013*\"stoat\" + 0.012*\"leicester\" + 0.012*\"satyr-countenance\"'),\n",
       " (49,\n",
       "  '0.037*\"short-wave\" + 0.028*\"satisfactorily\" + 0.025*\"satyr-countenance\" + 0.023*\"knifing\" + 0.019*\"shuddered\" + 0.019*\"last—though\" + 0.016*\"orient\" + 0.012*\"stoat\" + 0.011*\"masse\" + 0.010*\"snuff-and-butter\"'),\n",
       " (78,\n",
       "  '0.054*\"short-wave\" + 0.031*\"knifing\" + 0.023*\"shuddered\" + 0.023*\"satisfactorily\" + 0.020*\"last—though\" + 0.017*\"satyr-countenance\" + 0.014*\"leicester\" + 0.012*\"masse\" + 0.011*\"orient\" + 0.010*\"snuff-and-butter\"'),\n",
       " (84,\n",
       "  '0.043*\"short-wave\" + 0.023*\"last—though\" + 0.021*\"satisfactorily\" + 0.021*\"knifing\" + 0.018*\"satyr-countenance\" + 0.016*\"shuddered\" + 0.012*\"masse\" + 0.011*\"leicester\" + 0.010*\"orient\" + 0.009*\"snuff-and-butter\"'),\n",
       " (68,\n",
       "  '0.058*\"short-wave\" + 0.029*\"knifing\" + 0.027*\"satisfactorily\" + 0.021*\"last—though\" + 0.018*\"leicester\" + 0.016*\"shuddered\" + 0.016*\"orient\" + 0.015*\"satyr-countenance\" + 0.013*\"snuff-and-butter\" + 0.013*\"stoat\"'),\n",
       " (69,\n",
       "  '0.028*\"short-wave\" + 0.026*\"satisfactorily\" + 0.026*\"knifing\" + 0.015*\"shuddered\" + 0.015*\"leicester\" + 0.015*\"last—though\" + 0.013*\"masse\" + 0.012*\"satyr-countenance\" + 0.010*\"stoat\" + 0.010*\"snuff-and-butter\"'),\n",
       " (72,\n",
       "  '0.043*\"short-wave\" + 0.021*\"last—though\" + 0.020*\"satisfactorily\" + 0.018*\"knifing\" + 0.016*\"shuddered\" + 0.015*\"orient\" + 0.013*\"satyr-countenance\" + 0.011*\"masse\" + 0.010*\"bulleana\" + 0.009*\"leicester\"'),\n",
       " (85,\n",
       "  '0.059*\"short-wave\" + 0.039*\"knifing\" + 0.035*\"satisfactorily\" + 0.024*\"shuddered\" + 0.023*\"last—though\" + 0.017*\"satyr-countenance\" + 0.017*\"orient\" + 0.014*\"masse\" + 0.011*\"leicester\" + 0.010*\"suffers\"'),\n",
       " (67,\n",
       "  '0.058*\"short-wave\" + 0.033*\"satisfactorily\" + 0.031*\"knifing\" + 0.023*\"last—though\" + 0.019*\"orient\" + 0.016*\"shuddered\" + 0.015*\"masse\" + 0.013*\"stoat\" + 0.011*\"leicester\" + 0.010*\"satyr-countenance\"'),\n",
       " (73,\n",
       "  '0.058*\"short-wave\" + 0.033*\"satisfactorily\" + 0.029*\"knifing\" + 0.025*\"shuddered\" + 0.025*\"last—though\" + 0.016*\"masse\" + 0.015*\"orient\" + 0.015*\"satyr-countenance\" + 0.012*\"stoat\" + 0.011*\"leicester\"'),\n",
       " (98,\n",
       "  '0.041*\"short-wave\" + 0.031*\"knifing\" + 0.028*\"satyr-countenance\" + 0.019*\"shuddered\" + 0.018*\"last—though\" + 0.016*\"satisfactorily\" + 0.016*\"masse\" + 0.015*\"leicester\" + 0.015*\"orient\" + 0.013*\"snuff-and-butter\"'),\n",
       " (28,\n",
       "  '0.051*\"short-wave\" + 0.027*\"knifing\" + 0.021*\"satisfactorily\" + 0.019*\"satyr-countenance\" + 0.018*\"shuddered\" + 0.017*\"last—though\" + 0.016*\"snuff-and-butter\" + 0.011*\"masse\" + 0.010*\"leicester\" + 0.009*\"cheeses\"'),\n",
       " (39,\n",
       "  '0.069*\"short-wave\" + 0.031*\"satisfactorily\" + 0.029*\"knifing\" + 0.028*\"shuddered\" + 0.023*\"last—though\" + 0.018*\"satyr-countenance\" + 0.015*\"orient\" + 0.014*\"leicester\" + 0.013*\"bulleana\" + 0.010*\"stoat\"'),\n",
       " (16,\n",
       "  '0.078*\"short-wave\" + 0.028*\"last—though\" + 0.027*\"knifing\" + 0.026*\"satisfactorily\" + 0.024*\"shuddered\" + 0.016*\"masse\" + 0.014*\"satyr-countenance\" + 0.013*\"leicester\" + 0.012*\"orient\" + 0.011*\"snuff-and-butter\"'),\n",
       " (10,\n",
       "  '0.057*\"short-wave\" + 0.026*\"knifing\" + 0.021*\"satisfactorily\" + 0.018*\"last—though\" + 0.018*\"orient\" + 0.017*\"shuddered\" + 0.016*\"masse\" + 0.014*\"leicester\" + 0.011*\"satyr-countenance\" + 0.009*\"stoat\"'),\n",
       " (23,\n",
       "  '0.059*\"short-wave\" + 0.031*\"knifing\" + 0.028*\"satisfactorily\" + 0.024*\"shuddered\" + 0.022*\"last—though\" + 0.017*\"satyr-countenance\" + 0.015*\"snuff-and-butter\" + 0.013*\"leicester\" + 0.012*\"bulleana\" + 0.012*\"orient\"'),\n",
       " (43,\n",
       "  '0.048*\"short-wave\" + 0.027*\"knifing\" + 0.019*\"last—though\" + 0.019*\"shuddered\" + 0.018*\"satisfactorily\" + 0.013*\"satyr-countenance\" + 0.012*\"leicester\" + 0.012*\"bulleana\" + 0.011*\"masse\" + 0.010*\"snuff-and-butter\"'),\n",
       " (51,\n",
       "  '0.042*\"short-wave\" + 0.034*\"knifing\" + 0.025*\"shuddered\" + 0.023*\"satisfactorily\" + 0.020*\"satyr-countenance\" + 0.018*\"leicester\" + 0.018*\"last—though\" + 0.017*\"orient\" + 0.011*\"snuff-and-butter\" + 0.010*\"masse\"'),\n",
       " (34,\n",
       "  '0.069*\"short-wave\" + 0.033*\"satisfactorily\" + 0.025*\"last—though\" + 0.020*\"knifing\" + 0.015*\"orient\" + 0.015*\"shuddered\" + 0.014*\"stoat\" + 0.013*\"masse\" + 0.010*\"bulleana\" + 0.010*\"satyr-countenance\"'),\n",
       " (30,\n",
       "  '0.071*\"short-wave\" + 0.045*\"satisfactorily\" + 0.026*\"shuddered\" + 0.025*\"knifing\" + 0.020*\"orient\" + 0.020*\"last—though\" + 0.014*\"satyr-countenance\" + 0.013*\"masse\" + 0.011*\"snuff-and-butter\" + 0.011*\"leicester\"'),\n",
       " (66,\n",
       "  '0.056*\"short-wave\" + 0.029*\"knifing\" + 0.019*\"last—though\" + 0.018*\"satisfactorily\" + 0.016*\"shuddered\" + 0.014*\"masse\" + 0.013*\"orient\" + 0.012*\"satyr-countenance\" + 0.011*\"snuff-and-butter\" + 0.010*\"stoat\"'),\n",
       " (33,\n",
       "  '0.049*\"short-wave\" + 0.033*\"knifing\" + 0.023*\"satisfactorily\" + 0.022*\"last—though\" + 0.016*\"shuddered\" + 0.014*\"satyr-countenance\" + 0.013*\"leicester\" + 0.012*\"masse\" + 0.010*\"orient\" + 0.010*\"snuff-and-butter\"'),\n",
       " (17,\n",
       "  '0.048*\"short-wave\" + 0.031*\"knifing\" + 0.027*\"satisfactorily\" + 0.025*\"last—though\" + 0.021*\"satyr-countenance\" + 0.019*\"shuddered\" + 0.014*\"leicester\" + 0.014*\"masse\" + 0.012*\"snuff-and-butter\" + 0.012*\"justify\"'),\n",
       " (18,\n",
       "  '0.036*\"short-wave\" + 0.023*\"satisfactorily\" + 0.023*\"knifing\" + 0.021*\"last—though\" + 0.012*\"shuddered\" + 0.010*\"leicester\" + 0.010*\"masse\" + 0.010*\"orient\" + 0.008*\"stoat\" + 0.008*\"satyr-countenance\"'),\n",
       " (2,\n",
       "  '0.040*\"short-wave\" + 0.034*\"knifing\" + 0.029*\"last—though\" + 0.028*\"satisfactorily\" + 0.025*\"shuddered\" + 0.021*\"satyr-countenance\" + 0.019*\"orient\" + 0.016*\"leicester\" + 0.013*\"bulleana\" + 0.012*\"masse\"'),\n",
       " (20,\n",
       "  '0.051*\"short-wave\" + 0.026*\"satisfactorily\" + 0.025*\"knifing\" + 0.022*\"last—though\" + 0.021*\"shuddered\" + 0.020*\"satyr-countenance\" + 0.018*\"orient\" + 0.012*\"snuff-and-butter\" + 0.011*\"masse\" + 0.010*\"leicester\"'),\n",
       " (92,\n",
       "  '0.047*\"short-wave\" + 0.035*\"knifing\" + 0.022*\"last—though\" + 0.020*\"satyr-countenance\" + 0.019*\"shuddered\" + 0.019*\"satisfactorily\" + 0.013*\"orient\" + 0.013*\"leicester\" + 0.012*\"snuff-and-butter\" + 0.012*\"masse\"'),\n",
       " (75,\n",
       "  '0.049*\"short-wave\" + 0.029*\"knifing\" + 0.020*\"last—though\" + 0.018*\"satisfactorily\" + 0.017*\"satyr-countenance\" + 0.016*\"shuddered\" + 0.012*\"stoat\" + 0.012*\"leicester\" + 0.011*\"bulleana\" + 0.010*\"snuff-and-butter\"'),\n",
       " (37,\n",
       "  '0.081*\"short-wave\" + 0.027*\"satisfactorily\" + 0.025*\"last—though\" + 0.020*\"knifing\" + 0.020*\"leicester\" + 0.019*\"orient\" + 0.017*\"shuddered\" + 0.017*\"satyr-countenance\" + 0.015*\"snuff-and-butter\" + 0.012*\"masse\"'),\n",
       " (54,\n",
       "  '0.051*\"short-wave\" + 0.039*\"knifing\" + 0.023*\"last—though\" + 0.022*\"satisfactorily\" + 0.020*\"orient\" + 0.018*\"shuddered\" + 0.017*\"satyr-countenance\" + 0.014*\"leicester\" + 0.012*\"bulleana\" + 0.012*\"masse\"'),\n",
       " (15,\n",
       "  '0.054*\"short-wave\" + 0.032*\"satisfactorily\" + 0.027*\"knifing\" + 0.021*\"last—though\" + 0.019*\"shuddered\" + 0.017*\"orient\" + 0.012*\"masse\" + 0.011*\"satyr-countenance\" + 0.011*\"leicester\" + 0.009*\"stoat\"'),\n",
       " (22,\n",
       "  '0.034*\"short-wave\" + 0.022*\"satisfactorily\" + 0.021*\"knifing\" + 0.016*\"satyr-countenance\" + 0.014*\"last—though\" + 0.011*\"shuddered\" + 0.009*\"leicester\" + 0.009*\"masse\" + 0.009*\"orient\" + 0.008*\"snuff-and-butter\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling mit Mallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mallet Binary erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "basepath = os.path.abspath('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:18:43 DEBUG mallet: mallet_output/malletBinary.mallet\n",
      "26-Jan-2017 17:18:43 INFO mallet: Accessing Mallet ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mallet', 'import-dir', '--input', '/home/tv/git/Topics/corpus_txt', '--output', 'mallet_output/malletBinary.mallet', '--keep-sequence', '--remove-stopwords']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:18:43 DEBUG mallet: Mallet file available.\n"
     ]
    }
   ],
   "source": [
    "#path_to_mallet = \"/home/sina/Uni/Dariah/mallet/bin/mallet\"\n",
    "#path_to_mallet = \"mallet\"\n",
    "path_to_corpus = os.path.join(basepath, 'corpus_txt')\n",
    "assert os.path.exists(path_to_corpus)\n",
    "\n",
    "# FIXME woher kommt der Ausgabepfad? Das muss doch in irgendeinen Parameter?\n",
    "mal.create_mallet_binary(path_to_corpus)\n",
    "\n",
    "#toDo absolute Pfade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mallet output erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26-Jan-2017 17:18:43 DEBUG mallet: /home/tv/git/Topics/mallet_output\n",
      "26-Jan-2017 17:18:43 INFO mallet: Accessing Mallet ...\n",
      "26-Jan-2017 17:18:57 DEBUG mallet: Mallet file available.\n"
     ]
    }
   ],
   "source": [
    "outfolder = os.path.join(basepath, \"mallet_output\")\n",
    "malletBinary = os.path.join(outfolder, \"malletBinary.mallet\")\n",
    "mal.create_mallet_model(malletBinary, outfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"lda_model = 'out_easy/corpus.lda'\\ncorpus = 'out_easy/corpus.mm'\\ndictionary = 'out_easy/corpus.dict'\\ndoc_labels = 'out_easy/corpus_doclabels.txt'\\ninteractive  = False\\n\\nvis = visual.Visualization(lda_model, corpus, dictionary, doc_labels, interactive)\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''lda_model = 'out_easy/corpus.lda'\n",
    "corpus = 'out_easy/corpus.mm'\n",
    "dictionary = 'out_easy/corpus.dict'\n",
    "doc_labels = 'out_easy/corpus_doclabels.txt'\n",
    "interactive  = False\n",
    "\n",
    "vis = visual.Visualization(lda_model, corpus, dictionary, doc_labels, interactive)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#heatmap = visual.make_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visual.save_heatmap(\"./visualizations/heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis = collection.Visualization(lda_model, corpus, dictionary, doc_labels, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis.make_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vis.save_interactive(\"./visualizations/interactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ob ihr wirklich richtig steht, seht ihr, wenn ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![success](http://cdn2.hubspot.net/hub/128506/file-446943132-jpg/images/computer_woman_success.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
