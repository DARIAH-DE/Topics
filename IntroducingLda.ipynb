{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics â€“ Easy Topic Modeling in Python\n",
    "\n",
    "The text mining technique **Topic Modeling** has become a popular statistical method for clustering documents. This [Jupyter notebook](http://jupyter.org/) introduces a step-by-step workflow, basically containing data preprocessing, the actual topic modeling using **latent Dirichlet allocation** (LDA), which learns the relationships between words, topics, and documents, as well as some interactive visualizations to explore the model.\n",
    "\n",
    "LDA, introduced in the context of text analysis in [2003](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf), is an instance of a more general class of models called **mixed-membership models**. Involving a number of distributions and parameters, the topic model is typically performed using [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) with conjugate priors and is purely based on word frequencies.\n",
    "\n",
    "There have been written numerous introductions to topic modeling for humanists (e.g. [this one](http://scottbot.net/topic-modeling-for-humanists-a-guided-tour/)), which provide another level of detail regarding its technical and epistemic properties.\n",
    "\n",
    "For this workflow, you will need a corpus (a set of texts) as plain text (`.txt`) or [TEI XML](http://www.tei-c.org/index.xml) (`.xml`). Using the `dariah_topics` package, you also have the ability to process the output of [DARIAH-DKPro-Wrapper](https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper), a command-line tool for *natural language processing*.\n",
    "\n",
    "Topic modeling works best with very large corpora. The [TextGrid Repository](https://textgridrep.org/) is a great place to start searching for text data. Anyway, to demonstrate the technique, we provide one small text collection in the folder `grenzboten_sample` containing 15 diary excerpts, as well as 15 war diary excerpts, which appeared in *Die Grenzboten*, a German newspaper of the late 19th and early 20th century.\n",
    "\n",
    "**Of course, you can work with your own corpus in this notebook.**\n",
    "\n",
    "We're relying on the LDA implementation by [Allen B. Riddell](https://www.ariddell.org/), called [lda](http://pythonhosted.org/lda/index.html), which is very lightweight. Aside from that, we provide two more Jupyter notebooks:\n",
    "\n",
    "* [IntroducingMallet](IntroducingMallet.ipynb), using LDA by [MALLET](http://mallet.cs.umass.edu/topics.php), which is known to be very robust. \n",
    "* [IntroducingGensim](IntroducingGensim.ipynb), using LDA by called [Gensim](https://radimrehurek.com/project/gensim/), which is attractive because of its multi-core support.\n",
    "\n",
    "For more information in general, have a look at the [documentation](http://dev.digital-humanities.de/ci/job/DARIAH-Topics/doclinks/1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Installing dependencies\n",
    "\n",
    "To work within this Jupyter notebook, you will have to import the `dariah_topics` library. As you do, `dariah_topics` also imports a couple of external libraries, which have to be installed first. `pip` is the preferred installer program in Python. Starting with Python 3.4, it is included by default with the Python binary installers. If you are interested in `pip`, have a look at [this website](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "You have the ability to install dependencies via `pip` from within this notebook. To get a feeling for working with Jupyter, copy and paste (or best: type) the following code snippet in the empty cell below and press the **Run**-button.\n",
    "\n",
    "```\n",
    "import pip\n",
    "\n",
    "pip.main(['install', '-r', 'requirements.txt'])\n",
    "```\n",
    "\n",
    "If you get any errors or are not able to install *all* dependencies properly, try [Stack Overflow](https://stackoverflow.com/questions/tagged/pip) for troubleshooting or create a new issue on our [GitHub page](https://github.com/DARIAH-DE/Topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What have I done?\n",
    "\n",
    "With `import pip` you have imported the package `pip`, which is in the Python standard library included. In the next line, you called `pip`'s function `main` and commited a [list](https://en.wikipedia.org/wiki/List_(abstract_data_type) with three elements:\n",
    "\n",
    "1. `install` is the command to install packages.\n",
    "2. `-r` (or `--requirements`) installs from the given requirements file.\n",
    "3. `requirements.txt` is a simple text file containing a list of all required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final words\n",
    "As you already know, code has to be written in the grey cells. You execute a cell by clicking the **Run**-button. If you want to run all cells of the notebook at once, click **Cell > Run All** or **Kernel > Restart & Run All** respectively, if you want to restart the Python kernel first. On the left side of an (unexecuted) cell stands `In [ ]:`. The empty bracket means, that the cell hasn't been executed yet. By clicking **Run**, a star appears in the brackets (`In [*]:`), which means the process is running. In most cases, you won't see that star, because your computer is faster than your eyes. You can execute only one cell at once, all following executions will be in the waiting line. If the process of a cell is done, a number appears in the brackets (`In [1]:`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Starting with topic modeling!\n",
    "\n",
    "Execute the following cell to import modules from the `dariah_topics` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dariah_topics import preprocessing\n",
    "from dariah_topics import meta\n",
    "#from dariah_topics import postprocessing\n",
    "from dariah_topics import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will need some additional functions from external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show\n",
    "import lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not pay heed to any warnings right now and execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line will just tell the notebook to show graphics in the output frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Reading a corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the path to the corpus folder\n",
    "\n",
    "In the present example code, we are using the 30 diary excerpts from the folder `grenzboten`. To use your own corpus, change the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = 'grenzboten_sample/*.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding `/*.txt` to the actual path, we make sure to select only files with the suffix `.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing file paths and metadata\n",
    "We begin by creating a list of all the documents in the folder specified above. That list will tell the function `preprocessing.read_from_pathlist` (see below) which text documents to read. Furthermore, based on filenames we can create some metadata, e.g. author and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>basename</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beck</td>\n",
       "      <td>Beck_1844_Tagebuch_56</td>\n",
       "      <td>grenzboten_sample/Beck_1844_Tagebuch_56.txt</td>\n",
       "      <td>1844_Tagebuch_56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unbekannt</td>\n",
       "      <td>Unbekannt_1844_Tagebuch_70</td>\n",
       "      <td>grenzboten_sample/Unbekannt_1844_Tagebuch_70.txt</td>\n",
       "      <td>1844_Tagebuch_70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nimmer</td>\n",
       "      <td>Nimmer_1844_Tagebuch_77</td>\n",
       "      <td>grenzboten_sample/Nimmer_1844_Tagebuch_77.txt</td>\n",
       "      <td>1844_Tagebuch_77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unbekannt</td>\n",
       "      <td>Unbekannt_1844_Tagebuch_82</td>\n",
       "      <td>grenzboten_sample/Unbekannt_1844_Tagebuch_82.txt</td>\n",
       "      <td>1844_Tagebuch_82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JÃ¶rgel</td>\n",
       "      <td>JÃ¶rgel_1844_Tagebuch_88</td>\n",
       "      <td>grenzboten_sample/JÃ¶rgel_1844_Tagebuch_88.txt</td>\n",
       "      <td>1844_Tagebuch_88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      author                    basename  \\\n",
       "0       Beck       Beck_1844_Tagebuch_56   \n",
       "1  Unbekannt  Unbekannt_1844_Tagebuch_70   \n",
       "2     Nimmer     Nimmer_1844_Tagebuch_77   \n",
       "3  Unbekannt  Unbekannt_1844_Tagebuch_82   \n",
       "4     JÃ¶rgel     JÃ¶rgel_1844_Tagebuch_88   \n",
       "\n",
       "                                           filename             title  \n",
       "0       grenzboten_sample/Beck_1844_Tagebuch_56.txt  1844_Tagebuch_56  \n",
       "1  grenzboten_sample/Unbekannt_1844_Tagebuch_70.txt  1844_Tagebuch_70  \n",
       "2     grenzboten_sample/Nimmer_1844_Tagebuch_77.txt  1844_Tagebuch_77  \n",
       "3  grenzboten_sample/Unbekannt_1844_Tagebuch_82.txt  1844_Tagebuch_82  \n",
       "4     grenzboten_sample/JÃ¶rgel_1844_Tagebuch_88.txt  1844_Tagebuch_88  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = meta.fn2metadata(path_to_corpus)\n",
    "metadata[:5] # by adding '[:5]' to the variable, only the first 5 elements will be printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read listed documents from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tagebuch von Karl Beck. Man spricht seit vierzehn Tagen von einem vollstÃ¤ndigen Ministerwechsel und es circuliren im Publicum die verschiedensten Combinationen, wobei heute ganz andere Namen genannt werden, als gestern und morgen wieder andere, als heute.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(preprocessing.read_from_pathlist(metadata['filename']))\n",
    "corpus[0][:255] # printing the first 255 characters of the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `corpus` contains as much elements (`documents`) as texts in your corpus are. Each element of `corpus` is a list containing exactly one element, the text itself as one single string including all whitespaces and punctuations:\n",
    "\n",
    "```\n",
    "[['This is the content of your first document.'],\n",
    " ['This is the content of your second document.'],\n",
    " ...\n",
    " ['This is the content of your last document.']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tokenize corpus\n",
    "Now, your `documents` in `corpus` will be tokenized. Tokenization is the task of cutting a stream of characters into linguistic units, simply words or, more precisely, tokens. The tokenize function `dariah_topics` provides is a simple Unicode tokenizer. Depending on the corpus, it might be useful to use an external tokenizer function, or even develop your own, since its efficiency varies with language, epoch and text type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [list(preprocessing.tokenize(document)) for document in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, each `document` is represented by a list of separate token strings. As above, have a look at the first document (which has the index `0` as Python starts counting at 0) and show its first 14 words/tokens (that have the indices `0:13` accordingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagebuch',\n",
       " 'von',\n",
       " 'karl',\n",
       " 'beck',\n",
       " 'man',\n",
       " 'spricht',\n",
       " 'seit',\n",
       " 'vierzehn',\n",
       " 'tagen',\n",
       " 'von',\n",
       " 'einem',\n",
       " 'vollstÃ¤ndigen',\n",
       " 'ministerwechsel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0][0:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create a document-term matrix\n",
    "\n",
    "The LDA topic model is based on a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) of the corpus. In a document-term matrix, rows correspond to documents and columns correspond to terms or tokens respectively. The values are token frequencies for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>die</th>\n",
       "      <th>der</th>\n",
       "      <th>und</th>\n",
       "      <th>in</th>\n",
       "      <th>den</th>\n",
       "      <th>von</th>\n",
       "      <th>zu</th>\n",
       "      <th>das</th>\n",
       "      <th>des</th>\n",
       "      <th>nicht</th>\n",
       "      <th>...</th>\n",
       "      <th>staatsmonopol</th>\n",
       "      <th>steigernde</th>\n",
       "      <th>staatspapieren</th>\n",
       "      <th>staatsrÃ¼cksichten</th>\n",
       "      <th>staatszeitung</th>\n",
       "      <th>stallknecht</th>\n",
       "      <th>stammen</th>\n",
       "      <th>starkem</th>\n",
       "      <th>statu</th>\n",
       "      <th>subscriben</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_56</th>\n",
       "      <td>90.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_70</th>\n",
       "      <td>205.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_77</th>\n",
       "      <td>181.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_82</th>\n",
       "      <td>213.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_88</th>\n",
       "      <td>68.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    die    der    und     in   den   von     zu   das   des  \\\n",
       "1844_Tagebuch_56   90.0   92.0   84.0   70.0  30.0  26.0   25.0  16.0  25.0   \n",
       "1844_Tagebuch_70  205.0  224.0  193.0  133.0  78.0  64.0  112.0  86.0  45.0   \n",
       "1844_Tagebuch_77  181.0  153.0  141.0   98.0  55.0  59.0   72.0  59.0  44.0   \n",
       "1844_Tagebuch_82  213.0  207.0  169.0  128.0  85.0  86.0   79.0  80.0  66.0   \n",
       "1844_Tagebuch_88   68.0   59.0   70.0   54.0  22.0  27.0   28.0  19.0  13.0   \n",
       "\n",
       "                  nicht     ...      staatsmonopol  steigernde  \\\n",
       "1844_Tagebuch_56   23.0     ...                0.0         0.0   \n",
       "1844_Tagebuch_70   67.0     ...                0.0         0.0   \n",
       "1844_Tagebuch_77   41.0     ...                0.0         0.0   \n",
       "1844_Tagebuch_82   67.0     ...                0.0         0.0   \n",
       "1844_Tagebuch_88   11.0     ...                0.0         0.0   \n",
       "\n",
       "                  staatspapieren  staatsrÃ¼cksichten  staatszeitung  \\\n",
       "1844_Tagebuch_56             0.0                0.0            0.0   \n",
       "1844_Tagebuch_70             0.0                0.0            0.0   \n",
       "1844_Tagebuch_77             0.0                0.0            0.0   \n",
       "1844_Tagebuch_82             0.0                0.0            0.0   \n",
       "1844_Tagebuch_88             0.0                0.0            0.0   \n",
       "\n",
       "                  stallknecht  stammen  starkem  statu  subscriben  \n",
       "1844_Tagebuch_56          0.0      0.0      0.0    0.0         0.0  \n",
       "1844_Tagebuch_70          0.0      0.0      0.0    0.0         0.0  \n",
       "1844_Tagebuch_77          0.0      0.0      0.0    0.0         0.0  \n",
       "1844_Tagebuch_82          0.0      0.0      0.0    0.0         0.0  \n",
       "1844_Tagebuch_88          0.0      0.0      0.0    0.0         0.0  \n",
       "\n",
       "[5 rows x 24451 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix = preprocessing.create_document_term_matrix(tokenized_corpus, metadata['title'])\n",
    "document_term_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Feature removal\n",
    "\n",
    "*Stopwords* (also known as *most frequent tokens*) and *hapax legomena* are harmful for LDA and have to be removed from the corpus or the document-term matrix respectively. In this example, the 50 most frequent tokens will be categorized as stopwords.\n",
    "\n",
    "**Hint**: Be careful with removing most frequent tokens, you might remove tokens quite important for LDA. Anyway, to gain better results, it is highly recommended to use an external stopwords list.\n",
    "\n",
    "In this notebook, we combine the 50 most frequent tokens, hapax legomena and an external stopwordslist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the 100 most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = preprocessing.find_stopwords(document_term_matrix, most_frequent_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the five most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die', 'der', 'und', 'in', 'den']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List hapax legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of types in corpus: 24451\n",
      "Total number of hapax legomena: 19757\n"
     ]
    }
   ],
   "source": [
    "hapax_legomena = preprocessing.find_hapax_legomena(document_term_matrix)\n",
    "print(\"Total number of types in corpus:\", document_term_matrix.shape[1])\n",
    "print(\"Total number of hapax legomena:\", len(hapax_legomena))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Use external stopwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwordlist = \"tutorial_supplementals/stopwords/de.txt\"\n",
    "external_stopwords = [line.strip() for line in open(path_to_stopwordlist, 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine lists and remove content from `document_term_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = stopwords + hapax_legomena + external_stopwords\n",
    "document_term_matrix = preprocessing.remove_features(features, document_term_matrix=document_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is how your clean corpus looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>franzosen</th>\n",
       "      <th>genommen</th>\n",
       "      <th>abgewiesen</th>\n",
       "      <th>sÃ¼dlich</th>\n",
       "      <th>berlin</th>\n",
       "      <th>lassen</th>\n",
       "      <th>geschÃ¼tze</th>\n",
       "      <th>englische</th>\n",
       "      <th>januar</th>\n",
       "      <th>deutschland</th>\n",
       "      <th>...</th>\n",
       "      <th>bankprojects</th>\n",
       "      <th>ii&amp;gt</th>\n",
       "      <th>pflanzenausstellung</th>\n",
       "      <th>unbekÃ¼mmert</th>\n",
       "      <th>nischt</th>\n",
       "      <th>thiaumont-walde</th>\n",
       "      <th>kompositionen</th>\n",
       "      <th>monarchischer</th>\n",
       "      <th>ehrenwerthen</th>\n",
       "      <th>zaturcy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_70</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_77</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_82</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844_Tagebuch_88</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4242 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  franzosen  genommen  abgewiesen  sÃ¼dlich  berlin  lassen  \\\n",
       "1844_Tagebuch_56        0.0       1.0         0.0      0.0     4.0     3.0   \n",
       "1844_Tagebuch_70        2.0       1.0         0.0      0.0     8.0     6.0   \n",
       "1844_Tagebuch_77        0.0       1.0         0.0      0.0     5.0     9.0   \n",
       "1844_Tagebuch_82        0.0       0.0         0.0      0.0     5.0    10.0   \n",
       "1844_Tagebuch_88        2.0       0.0         0.0      0.0     0.0     1.0   \n",
       "\n",
       "                  geschÃ¼tze  englische  januar  deutschland   ...     \\\n",
       "1844_Tagebuch_56        0.0        0.0     0.0          4.0   ...      \n",
       "1844_Tagebuch_70        0.0        3.0     0.0          1.0   ...      \n",
       "1844_Tagebuch_77        0.0        2.0     2.0          2.0   ...      \n",
       "1844_Tagebuch_82        0.0        0.0     0.0          2.0   ...      \n",
       "1844_Tagebuch_88        0.0        0.0     0.0          4.0   ...      \n",
       "\n",
       "                  bankprojects  ii&gt  pflanzenausstellung  unbekÃ¼mmert  \\\n",
       "1844_Tagebuch_56           0.0    0.0                  0.0          0.0   \n",
       "1844_Tagebuch_70           0.0    0.0                  0.0          2.0   \n",
       "1844_Tagebuch_77           0.0    0.0                  0.0          0.0   \n",
       "1844_Tagebuch_82           0.0    0.0                  0.0          0.0   \n",
       "1844_Tagebuch_88           0.0    0.0                  0.0          0.0   \n",
       "\n",
       "                  nischt  thiaumont-walde  kompositionen  monarchischer  \\\n",
       "1844_Tagebuch_56     0.0              0.0            0.0            0.0   \n",
       "1844_Tagebuch_70     0.0              0.0            0.0            0.0   \n",
       "1844_Tagebuch_77     0.0              0.0            0.0            0.0   \n",
       "1844_Tagebuch_82     0.0              0.0            0.0            0.0   \n",
       "1844_Tagebuch_88     0.0              0.0            0.0            0.0   \n",
       "\n",
       "                  ehrenwerthen  zaturcy  \n",
       "1844_Tagebuch_56           0.0      0.0  \n",
       "1844_Tagebuch_70           0.0      0.0  \n",
       "1844_Tagebuch_77           0.0      0.0  \n",
       "1844_Tagebuch_82           0.0      0.0  \n",
       "1844_Tagebuch_88           0.0      0.0  \n",
       "\n",
       "[5 rows x 4242 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model creation\n",
    "\n",
    "The actual topic modeling is done with external state-of-the-art LDA implementations. In this example, we are relying on the open-source toolkit **lda**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate numbers back into words after the model creation, you have to set up a list of all unique tokens in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['franzosen', 'genommen', 'abgewiesen', 'sÃ¼dlich', 'berlin', 'lassen',\n",
       "       'geschÃ¼tze', 'englische', 'januar', 'deutschland',\n",
       "       ...\n",
       "       'bankprojects', 'ii&gt', 'pflanzenausstellung', 'unbekÃ¼mmert', 'nischt',\n",
       "       'thiaumont-walde', 'kompositionen', 'monarchischer', 'ehrenwerthen',\n",
       "       'zaturcy'],\n",
       "      dtype='object', length=4242)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = document_term_matrix.columns\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Translate document-term matrix into an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, all values of your document-term matrix will be translated into an [array](https://en.wikipedia.org/wiki/Array_data_structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0, ...,  0,  0,  0],\n",
       "       [ 2,  1,  0, ...,  0,  0,  0],\n",
       "       [ 0,  1,  0, ...,  0,  0,  0],\n",
       "       ..., \n",
       "       [12, 17, 11, ...,  0,  0,  0],\n",
       "       [12,  6,  9, ...,  0,  0,  0],\n",
       "       [ 6,  6, 19, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_matrix_arr = document_term_matrix.as_matrix().astype(int)\n",
    "document_term_matrix_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Generate LDA model\n",
    "\n",
    "We use the class `LDA` from the library `lda` (which is basically not the same, because Python is case sensitive) to generate a LDA topic model. To instance a `LDA` object, there have to be specified a couple of parameters.\n",
    "\n",
    "But first, if you are curious about any library, module, class or function, try `help()`. This can be very useful, because (at least in a well documented library) explanations of use and parameters will be printed. We're interested in the class `LDA` of the library `lda`, so let's try:\n",
    "\n",
    "```\n",
    "help(lda.LDA)\n",
    "```\n",
    "\n",
    "This will print something like this (in fact even more):\n",
    "\n",
    "```\n",
    "Help on class LDA in module lda.lda:\n",
    "\n",
    "class LDA(builtins.object)\n",
    " |  Latent Dirichlet allocation using collapsed Gibbs sampling\n",
    " |  \n",
    " |  Parameters\n",
    " |  ----------\n",
    " |  n_topics : int\n",
    " |      Number of topics\n",
    " |  \n",
    " |  n_iter : int, default 2000\n",
    " |      Number of sampling iterations\n",
    " |  \n",
    " |  alpha : float, default 0.1\n",
    " |      Dirichlet parameter for distribution over topics\n",
    " |  \n",
    " |  eta : float, default 0.01\n",
    " |      Dirichlet parameter for distribution over words\n",
    " |  \n",
    " |  random_state : int or RandomState, optional\n",
    " |      The generator used for the initial topics.\n",
    "```\n",
    "\n",
    "So, now you know how to define the number of topics and the number of sampling iterations as well. A higher number of iterations will probably yield a better model, but also increases processing time. `alpha`, `eta` and `random_state` are so-called *hyperparameters*. They influence the model's performance, so feel free to play around with them. In the present example, we will leave the default values. Furthermore, there exist various methods for hyperparameter optimization, e.g. gridsearch or Gaussian optimization.\n",
    "\n",
    "**Warning: This step can take quite a while!** Meaning something between some seconds and some hours depending on corpus size and the number of iterations. Our example corpus should be done within a minute or two at `n_iter=5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.6 s, sys: 17.9 ms, total: 53.7 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=5000)\n",
    "model.fit(document_term_matrix_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Create document-topic matrix\n",
    "\n",
    "The generated model object can now be translated into a human-readable document-topic matrix (that is a actually a pandas DataFrame) that constitutes our principle exchange format for topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#topics = postprocessing.show_topics(document_term_matrix, model)\n",
    "#topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following matrix contains the probability per topic for each document, which we need for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_topics = postprocessing.show_document_topics(model, topics, metadata['title'])\n",
    "#document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Distribution of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics over all documents\n",
    "\n",
    "The distribution of topics over all documents can now be visualized in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = visualization.plot_document_topics_heatmap(doc_topics, interactive=True)\n",
    "#show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of topics in a single documents\n",
    "\n",
    "To take closer look on the topics in a single text, we can use the follwing function that shows all the topics in a text and their respective proportions. To select the document, we have to give its index to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualization.plot_doc_topics(doc_topics, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
