{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for using the fancy_name_toolbox_for_mighty_topic_Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorial will explain the usage of the python library \"cophi_toolbox\". If you have not done so, please follow the instructions installing jupyter and all python libraries mentioned in readme.txt/installation_instructions.\n",
    "\n",
    "The toolbox provides two different approaches for preprocessing which both lead to tokenised text. After that you can use one of two topic modelling algorithms to create topic models for your corpus. The last step will be visualising the results.\n",
    "In topic modelling preprocessing the data your topic modelling algorithm will handle is a mayor concern. Different kind of text will react differently to certain preprocessing steps and going furter, adapting the preprocessing is crucial for better topic model. Therefore, the cophi_toolbox can either take plain text(Step 1-?) or already nlp enhanced csv file(Step ? to ?) as input. For now the csv input is specified to use dkproWrapper output. For more information follow this link: https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/tree/master/doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Importing packages\n",
    "Firstly, we get access to the functionalities of the toolbox by importing them. For using its functions we use the prefix of the toolbox's submodules (pre, visual and mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3685325fe773>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3685325fe773>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from /Users/MHuber/git/DariahTopics/dariah_topics import preprocessing as pre\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dariah_topics import preprocessing as pre\n",
    "from dariah_topics import visualization as visual\n",
    "from dariah_topics import mallet as mal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loading txt-Files\n",
    "We start with usiung plain text files, if you want to use nlp annotated data(csv files), skip to Step ?\n",
    "\n",
    "With the second step we load the plain text corpous into memory for further preprocessing. The Tutorial includes an example set of data. Should you want to use your own corpus, change the path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a list of documents in the directory (in this case 'corpus_txt', a sample text corpus)\n",
    "\n",
    "path_txt = \"corpus_txt\"\n",
    "\n",
    "doclist_txt = pre.create_document_list(path_txt)\n",
    "\n",
    "doclist_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a list of filenames used for the datamodel and the visualisation\n",
    "doc_labels = list(pre.get_labels(doclist_txt))\n",
    "doc_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads/yields the text files of the corpus\n",
    "\n",
    "corpus_txt = pre.read_from_txt(doclist_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenising\n",
    "In this step we tokenise the text (into words), as topic modelling algorithms usually work on a bag-of-words model. The tokenise function the toolbox provides is a simple unicode tokeniser. Depending on the corpus it might be useful to import you own tokeniser since the efficiency varies language, epoch etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizes the corpus and returns a list of tokens of one text as an example\n",
    "\n",
    "doc_tokens = [list(pre.tokenize(txt)) for txt in list(corpus_txt)]\n",
    "doc_tokens[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Working with dkproWrapper output\n",
    "As mentioned above, you need to run the code listed in this Step to incorporate csv-Output created by dkproWrapper.\n",
    "The output is already tokenised. Therefore it is unecessary to go through Step 3 \"Tokenising\".\n",
    "As in step 2, we need a list of document names.\n",
    "With the function provided the program reads only the most viable information from the csv file as default. You can pass an addition parameter with a list of column names(to read_from_csv; has to be of type 'list') to specify the information you retrieve.\n",
    "As default, the filter_csv selects lemmas with the POS-Tags for adjective, verb and noun. By passing an argument (to filter_csv; has to be of type 'list') you can specify the value of the POS-Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a list of documents in the directory (in this case 'corpus_csv', a sample dkproWrapper output)\n",
    "path_csv = \"corpus_csv\"\n",
    "\n",
    "doclist_csv = pre.create_document_list(path_csv, 'csv')\n",
    "doclist_csv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a list of filenames used for the datamodel and the visualisation\n",
    "doc_labels = list(pre.get_labels(doclist_txt))\n",
    "doc_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads/yields the text files of the corpus. Only reads columns 'ParagraphId', 'TokenId', 'Lemma', 'CPOS', 'NamedEntity'\n",
    "corpus_csv = pre.read_from_csv(doclist_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# needs editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filters the dkproWrapper output to only add the lemmas of certain POS-Tags. (adjectives, verbs and nouns by default)\n",
    "doc_tokens = #transform filter_csv into list of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transforming the data\n",
    "With the code in the following sections the text corpus is transformed into a data structure similar to the used in gensim. This new data model allows a more efficient processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transforms the corpus into a marketmatrix\n",
    "id_types, doc_ids = pre.create_dictionaries(doc_labels, doc_tokens)\n",
    "sparse_bow = pre.create_mm(doc_labels, doc_tokens, id_types, doc_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look at data structure by running the section below, you can see that each document and each word type of the corpus is represented by a number (column doc_id and token_id). In turn, each token_id has asigned a word count in the adjacent column, showing how often the word with asigned token_id occurs in the document no. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shows the datamodel\n",
    "sparse_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Removing features\n",
    "In this step we show several steps to populate a list of words, we want to ignore for the topic modelling step. One scenario could be to remove the most frequent words, as these are usually function words bare of semantic meaning. Another possibility is to remove hapax legomena (words occuring only once) which are usually considered noise in a topic model. The third is to simply use a stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reads a txt file (one word per line) and creates a list\n",
    "import os.path\n",
    "basepath = os.path.abspath('.')\n",
    "\n",
    "with open(os.path.join(basepath, \"tutorial_supplementals\", \"stopwords\", \"en\"), 'r', encoding = 'utf-8') as f: \n",
    "    stopword_list = f.read().split('\\n')\n",
    "    \n",
    "stopword_list = set(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removes stopwords\n",
    "sparse_df_stopwords_removed = pre.remove_features(sparse_bow, id_types, stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a stopword list consisting of the 100 most frequent words\n",
    "stopword_list = pre.find_stopwords(sparse_bow, id_types, 100)\n",
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a list of words only occuring once per document\n",
    "hapax_list = pre.find_hapax(sparse_bow, id_types)\n",
    "len(hapax_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combines list of most frequent words and words occuring only once into one list\n",
    "#cleans the corpus of the combined list\n",
    "feature_list = set(stopword_list).union(hapax_list)\n",
    "clean_term_frequency = pre.remove_features(sparse_bow, id_types, feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Topic modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code tranforms the data structure we created earlier into the format used by Gensim and saves it on disk. Following this code block we create the topic model. Depending on the size of the corpus used for it, lean back and get a coffee. Or a pot. A biig pot. Or go home and get some sleep :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports gensim functions\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_docs = max(sparse_bow.index.get_level_values(\"doc_id\"))\n",
    "num_types = max(sparse_bow.index.get_level_values(\"token_id\"))\n",
    "sum_counts = sum(sparse_bow[0])\n",
    "\n",
    "header_string = str(num_docs) + \" \" + str(num_types) + \" \" + str(sum_counts) + \"\\n\"\n",
    "\n",
    "with open(\"gb_plain.mm\", 'w', encoding = \"utf-8\") as f:\n",
    "    pass\n",
    "\n",
    "with open(\"gb_plain.mm\", 'a', encoding = \"utf-8\") as f:\n",
    "    f.write(\"%%MatrixMarket matrix coordinate real general\\n\")\n",
    "    f.write(header_string)\n",
    "    sparse_bow.to_csv( f, sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "mm = MmCorpus(\"gb_plain.mm\")\n",
    "doc2id = {value : key for key, value in doc_ids.items()}\n",
    "type2id = {value : key for key, value in id_types.items()}\n",
    "\n",
    "model = LdaModel(corpus=mm, id2word=type2id, num_topics=60, alpha = \"symmetric\", passes = 10)\n",
    "\n",
    "topic_nr_x = model.get_topic_terms(10)\n",
    "\n",
    "[type2id[i[0]] for i in topic_nr_x]\n",
    "\n",
    "topics = model.show_topics(num_topics = 60)\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Topic Modelling with mallet\n",
    "Another algorithm for topic modelling is implemented in the java-based software mallet. For this to work you need to download and install mallet from http://mallet.cs.umass.edu/download.php.\n",
    "Mallet uses plain text as input, so none of the preprocessing of this package is available for mallet topic modelling as of yet.\n",
    "To use mallet for topic modelling via this Python Script, you need to pass paths to the mallet binary, input and output. Again, depending on the size of your corpus, after calling mal.create_MalletMatrix() you should get coffee... or tea, a lot of tea, a huge pot, believe me, it's great, it's fantastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sets path to corpus and mallet binary and creates an output folder\n",
    "path_to_corpus = os.path.join(os.path.abspath('.'), 'corpus_txt')\n",
    "path_to_mallet = \"insert_path_here\"\n",
    "malletBinary = mal.create_mallet_binary(path_to_corpus, path_to_mallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sets Path to the output folder and works its magic\n",
    "basepath = os.path.join(os.path.abspath('.'), \"tutorial_supplementals/mallet_output\")\n",
    "doc_topics = os.path.join(basepath, \"doc_topics.txt\")\n",
    "\n",
    "\n",
    "mal.create_MalletMatrix(doc_topics)\n",
    "\n",
    "'''\n",
    "         ,/   *\n",
    "      _,'/_   |\n",
    "      `(\")' ,'/\n",
    "   _ _,-H-./ /\n",
    "   \\_\\_\\.   /\n",
    "     )\" |  (\n",
    "  __/   H   \\__\n",
    "  \\    /|\\    /\n",
    "   `--'|||`--'\n",
    "      ==^==\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
