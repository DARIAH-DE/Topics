{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `collection.py`\n",
    "The following tutorial shows how to use the `collection` module of DARIAH-Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prearrangement\n",
    "First you need to import the collection module so your IPython notebook has access to its functions and classes. As second step we set paths for a test corpus consisting of plain text files and one consisting of annotated text preprocessed with several NLP-Tools in form of CSV files (if you have questions concerning the format, click [here](https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper/blob/master/doc/tutorial.adoc))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_txt = \"corpus_txt\"\n",
    "path_csv = \"corpus_csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating list of filenames (plain text and csv files)\n",
    "The following function is used to normalize path names so non-uniform text files will be processable by the module. It is possible to add an additional argument `ext` where you can specify an extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:13 INFO collection: Creating document list from TXT files ...\n",
      "29-Nov-2016 12:02:13 DEBUG collection: 17 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_txt/Doyle_AScandalinBohemia.txt',\n",
       " 'corpus_txt/Doyle_AStudyinScarlet.txt',\n",
       " 'corpus_txt/Doyle_TheHoundoftheBaskervilles.txt',\n",
       " 'corpus_txt/Doyle_TheSignoftheFour.txt',\n",
       " 'corpus_txt/Howard_GodsoftheNorth.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doclist_txt = collection.create_document_list(path_txt)\n",
    "doclist_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:15 INFO collection: Creating document list from CSV files ...\n",
      "29-Nov-2016 12:02:15 DEBUG collection: 16 entries in document list.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpus_csv/Doyle_AStudyinScarlet.txt.csv',\n",
       " 'corpus_csv/Doyle_TheHoundoftheBaskervilles.txt.csv',\n",
       " 'corpus_csv/Doyle_TheSignoftheFour.txt.csv',\n",
       " 'corpus_csv/Howard_GodsoftheNorth.txt.csv',\n",
       " 'corpus_csv/Howard_SchadowsinZamboula.txt.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doclist_csv = collection.create_document_list(path_csv, 'csv')\n",
    "doclist_csv[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting document labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:16 INFO collection: Creating document labels ...\n",
      "29-Nov-2016 12:02:16 DEBUG collection: Document labels available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Doyle_AScandalinBohemia',\n",
       " 'Doyle_AStudyinScarlet',\n",
       " 'Doyle_TheHoundoftheBaskervilles',\n",
       " 'Doyle_TheSignoftheFour',\n",
       " 'Howard_GodsoftheNorth']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_labels = collection.get_labels(doclist_txt)\n",
    "list(doc_labels)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the corpora and optional e.g. stopwords list\n",
    "By using the `read_from()`-functions we create a generator object which provides a memory efficient way to handle bigger corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_txt = collection.read_from_txt(doclist_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_csv = collection.read_from_csv(doclist_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "external = collection.read_from_txt(\"helpful_stuff/stopwords/en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Segmenting text\n",
    "An important part of pre-processing in Topic Modeling is segmenting the the texts in 'chunks'. The arguments of the function are for the targeted corpus and the size of the 'chunks' in words. Depending on the languange and type of text results can vary widely in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:21 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:21 INFO collection: Segmenting document ...\n",
      "29-Nov-2016 12:02:21 DEBUG collection: Segment has a length of 1000 characters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A SCANDAL IN BOHEMIA\\n\\nA. CONAN DOYLE\\n\\n\\nI\\n\\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\\nmention her under any other name. In his eyes she eclipses and\\npredominates the whole of her sex. It was not that he felt any emotion\\nakin to love for Irene Adler. All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind. He was,\\nI take it, the most perfect reasoning and observing machine that the\\nworld has seen; but as a lover, he would have placed himself in a false\\nposition. He never spoke of the softer passions, save with a gibe and a\\nsneer. They were admirable things for the observer--excellent for\\ndrawing the veil from men's motives and actions. But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results. Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, w\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = collection.segmenter(corpus_txt, 1000)\n",
    "next(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Filtering text using POS-Tags\n",
    "Another way to preprocess the text is by filtering by POS-Tags and using lemmas (in this case only adjectives, verbs and nouns are filterable). The annotated CSV-file we provide in this example is already enriched with this kind of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:23 INFO collection: Accessing CSV documents ...\n",
      "29-Nov-2016 12:02:23 INFO collection: Accessing ['ADJ', 'V', 'NN'] lemmas ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37    typographical\n",
       "56          textual\n",
       "59           square\n",
       "72              old\n",
       "75             such\n",
       "Name: Lemma, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = collection.filter_POS_tags(corpus_csv)\n",
    "next(lemmas)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Count terms to use `find_stopwords()` and `find_hapax()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:25 INFO collection: Calculating term frequency ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:02:25 DEBUG collection: Term frequency calculated.\n"
     ]
    }
   ],
   "source": [
    "corpus = collection.calculate_term_frequency(corpus_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_corpus = corpus.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:30 INFO collection: Finding stopwords ...\n",
      "29-Nov-2016 12:02:30 DEBUG collection: 50 stopwords found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "the    21357\n",
       "of     11614\n",
       "and    11040\n",
       "to      8516\n",
       "a       7652\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = collection.find_stopwords(corpus, 50)\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Find hapax legomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:33 INFO collection: Find hapax legomena ...\n",
      "29-Nov-2016 12:02:33 DEBUG collection: 26096 hapax legomena found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'--Un     1\n",
       "\"'About    1\n",
       "\"'After    1\n",
       "\"'All      1\n",
       "\"'An       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapax = collection.find_hapax(corpus)\n",
    "hapax[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:02:46 INFO collection: Removing features ...\n",
      "29-Nov-2016 12:02:46 DEBUG collection: 50 features removed.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus = collection.remove_features(clean_corpus, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:03:01 INFO collection: Removing features ...\n",
      "29-Nov-2016 12:06:21 DEBUG collection: 26096 features removed.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus = collection.remove_features(clean_corpus, hapax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:21 INFO collection: Removing features ...\n",
      "29-Nov-2016 12:06:21 DEBUG collection: Accessing TXT document ...\n",
      "29-Nov-2016 12:06:22 DEBUG collection: 444 features removed.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus = collection.remove_features(clean_corpus, external)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des Kropus:  43989\n",
      "Länge des Korpus (ohne features):  17399\n",
      "5 MFWs:\n",
      " the    21357\n",
      "of     11614\n",
      "and    11040\n",
      "to      8516\n",
      "a       7652\n",
      "dtype: int64 \n",
      "\n",
      "5 MFWs (ohne features):\n",
      " will    841\n",
      "It      789\n",
      "We      601\n",
      "man     511\n",
      "\"I      496\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Länge des Kropus: \", len(corpus))\n",
    "print(\"Länge des Korpus (ohne features): \", len(clean_corpus))\n",
    "print(\"5 MFWs:\\n\", corpus.sort_values(ascending=False).head(5), \"\\n\")\n",
    "print(\"5 MFWs (ohne features):\\n\", clean_corpus.sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Matrix Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9c002f252a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatrix_market\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_matrix_market\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/severin/git/Topics/collection.py\u001b[0m in \u001b[0;36mcreate_matrix_market\u001b[0;34m(clean_term_frequency, doc_labels)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# and now we make our words list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mallwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_term_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0malldocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matrix_market = collection.create_matrix_market(corpus, doc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization\n",
    "Simple get-functions are implemented for visualization tasks. In this case the get_labels-function extracts the titles of the corpus files we loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:37 INFO collection: Accessing corpus ...\n",
      "29-Nov-2016 12:06:37 INFO gensim.corpora.indexedcorpus: loaded corpus index from out_easy/corpus.mm.index\n",
      "29-Nov-2016 12:06:37 INFO gensim.matutils: initializing corpus reader from out_easy/corpus.mm\n",
      "29-Nov-2016 12:06:37 INFO gensim.matutils: accepted corpus with 17 documents, 514 features, 4585 non-zero entries\n",
      "29-Nov-2016 12:06:37 DEBUG collection: Corpus available.\n",
      "29-Nov-2016 12:06:37 INFO collection: Accessing model ...\n",
      "29-Nov-2016 12:06:37 INFO gensim.utils: loading LdaModel object from out_easy/corpus.lda\n",
      "29-Nov-2016 12:06:37 INFO gensim.utils: loading id2word recursively from out_easy/corpus.lda.id2word.* with mmap=None\n",
      "29-Nov-2016 12:06:37 INFO gensim.utils: setting ignored attribute state to None\n",
      "29-Nov-2016 12:06:37 INFO gensim.utils: setting ignored attribute dispatcher to None\n",
      "29-Nov-2016 12:06:37 INFO gensim.utils: loading LdaModel object from out_easy/corpus.lda.state\n",
      "29-Nov-2016 12:06:37 DEBUG collection: Model available.\n",
      "29-Nov-2016 12:06:37 DEBUG collection: :param: interactive == False.\n",
      "29-Nov-2016 12:06:37 INFO collection: Accessing doc_labels ...\n",
      "29-Nov-2016 12:06:37 DEBUG collection: doc_labels accessed.\n",
      "29-Nov-2016 12:06:37 DEBUG collection: 29 doc_labels available.\n",
      "29-Nov-2016 12:06:37 DEBUG collection: Corpus, model and doc_labels available.\n"
     ]
    }
   ],
   "source": [
    "lda_model = 'out_easy/corpus.lda'\n",
    "corpus = 'out_easy/corpus.mm'\n",
    "dictionary = 'out_easy/corpus.dict'\n",
    "doc_labels = 'out_easy/corpus_doclabels.txt'\n",
    "interactive  = False\n",
    "\n",
    "vis = collection.Visualization(lda_model, corpus, dictionary, doc_labels, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:39 INFO collection: Accessing topic distribution and topic probability ...\n",
      "29-Nov-2016 12:06:39 DEBUG collection: Topic distribution and topic probability available.\n",
      "29-Nov-2016 12:06:39 INFO collection: Accessing plot labels ...\n",
      "29-Nov-2016 12:06:39 DEBUG collection: 10 plot labels available.\n",
      "29-Nov-2016 12:06:39 INFO collection: Creating heatmap figure ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing only...\n",
      "corpus.mm:\n",
      "[(0, 2.0), (1, 2.0), (2, 3.0), (3, 2.0), (4, 8.0), (5, 2.0), (6, 3.0), (7, 2.0), (8, 4.0), (9, 5.0), (10, 14.0), (11, 4.0), (12, 4.0), (13, 4.0), (14, 2.0), (15, 2.0), (16, 11.0), (17, 34.0), (18, 3.0), (19, 3.0), (20, 4.0), (21, 2.0), (22, 2.0), (23, 2.0), (24, 6.0), (25, 2.0), (26, 3.0), (27, 3.0), (28, 3.0), (29, 7.0), (30, 2.0), (31, 2.0), (32, 2.0), (33, 7.0), (34, 2.0), (35, 2.0), (36, 11.0), (37, 3.0), (38, 2.0), (39, 3.0), (40, 2.0), (41, 2.0), (42, 3.0), (43, 2.0), (44, 2.0), (45, 2.0), (46, 8.0), (47, 2.0), (48, 3.0), (49, 2.0), (50, 2.0), (51, 2.0), (52, 4.0), (53, 4.0), (54, 3.0), (55, 2.0), (56, 2.0), (57, 2.0), (58, 3.0), (59, 3.0), (60, 3.0), (61, 2.0), (62, 3.0), (63, 9.0), (64, 3.0), (65, 3.0), (66, 3.0), (67, 5.0), (68, 3.0), (69, 11.0), (70, 4.0), (71, 2.0), (72, 6.0), (73, 2.0), (74, 2.0), (75, 2.0), (76, 3.0), (77, 6.0), (78, 3.0), (79, 3.0), (80, 2.0), (81, 3.0), (82, 2.0), (83, 4.0), (84, 2.0), (85, 4.0), (86, 8.0), (87, 8.0), (88, 2.0), (89, 6.0), (90, 2.0), (91, 2.0), (92, 2.0), (93, 2.0), (94, 2.0), (95, 4.0), (96, 2.0), (97, 4.0), (98, 2.0), (99, 5.0), (100, 3.0), (101, 3.0), (102, 2.0), (103, 2.0), (104, 5.0), (105, 2.0), (106, 3.0), (107, 3.0), (108, 4.0), (109, 2.0), (110, 18.0), (111, 3.0), (112, 2.0), (113, 3.0), (114, 2.0), (115, 4.0), (116, 3.0), (117, 2.0), (118, 2.0), (119, 3.0), (120, 5.0), (121, 2.0), (122, 5.0), (123, 2.0), (124, 2.0), (125, 2.0), (126, 2.0), (127, 2.0), (128, 3.0), (129, 2.0), (130, 4.0), (131, 2.0), (132, 2.0), (133, 2.0), (134, 2.0), (135, 3.0), (136, 2.0), (137, 4.0), (138, 2.0), (139, 3.0), (140, 3.0), (141, 2.0), (142, 2.0), (143, 3.0), (144, 2.0), (145, 7.0), (146, 3.0), (147, 3.0), (148, 2.0), (149, 2.0), (150, 2.0), (151, 6.0), (152, 17.0), (153, 2.0), (154, 2.0), (155, 3.0), (156, 3.0), (157, 2.0), (158, 3.0), (159, 2.0), (160, 13.0), (161, 4.0), (162, 15.0), (163, 3.0), (164, 2.0), (165, 2.0), (166, 3.0), (167, 2.0), (168, 2.0), (169, 3.0), (170, 4.0), (171, 2.0), (172, 12.0), (173, 2.0), (174, 2.0), (175, 2.0), (176, 2.0), (177, 2.0), (178, 5.0), (179, 2.0), (180, 11.0), (181, 22.0), (182, 4.0), (183, 2.0), (184, 2.0), (185, 10.0), (186, 2.0), (187, 2.0), (188, 2.0), (189, 2.0), (190, 3.0), (191, 2.0), (192, 2.0), (193, 2.0), (194, 2.0), (195, 8.0), (196, 2.0), (197, 3.0), (198, 2.0), (199, 2.0), (200, 4.0), (201, 5.0), (202, 3.0), (203, 6.0), (204, 2.0), (205, 2.0), (206, 4.0), (207, 2.0), (208, 6.0), (209, 2.0), (210, 3.0), (211, 3.0), (212, 2.0), (213, 4.0), (214, 2.0), (215, 2.0), (216, 3.0), (217, 3.0), (218, 4.0), (219, 5.0), (220, 2.0), (221, 18.0), (222, 3.0), (223, 2.0), (224, 2.0), (225, 3.0), (226, 3.0), (227, 2.0), (228, 3.0), (229, 3.0), (230, 2.0), (231, 2.0), (232, 48.0), (233, 2.0), (234, 2.0), (235, 2.0), (236, 5.0), (237, 2.0), (238, 3.0), (239, 2.0), (240, 2.0), (241, 6.0), (242, 3.0), (243, 6.0), (244, 4.0), (245, 2.0), (246, 2.0), (247, 5.0), (248, 2.0), (249, 4.0), (250, 2.0), (251, 2.0), (252, 2.0), (253, 3.0), (254, 5.0), (255, 5.0), (256, 2.0), (257, 10.0), (258, 2.0), (259, 3.0), (260, 5.0), (261, 2.0), (262, 4.0), (263, 2.0), (264, 2.0), (265, 2.0), (266, 21.0), (267, 2.0), (268, 6.0), (269, 2.0), (270, 2.0), (271, 7.0), (272, 2.0), (273, 2.0), (274, 3.0), (275, 3.0), (276, 4.0), (277, 2.0), (278, 3.0), (279, 4.0), (280, 2.0), (281, 2.0), (282, 2.0), (283, 4.0), (284, 2.0), (285, 2.0), (286, 2.0), (287, 2.0), (288, 2.0), (289, 2.0), (290, 3.0), (291, 2.0), (292, 3.0), (293, 3.0), (294, 3.0), (295, 4.0), (296, 2.0), (297, 2.0), (298, 4.0), (299, 3.0), (300, 2.0), (301, 3.0), (302, 4.0), (303, 4.0), (304, 4.0), (305, 3.0), (306, 2.0), (307, 3.0), (308, 3.0), (309, 4.0), (310, 5.0), (311, 4.0), (312, 6.0), (313, 13.0), (314, 2.0), (315, 2.0), (316, 3.0), (317, 3.0), (318, 2.0), (319, 2.0), (320, 2.0), (321, 2.0), (322, 2.0), (323, 9.0), (324, 3.0), (325, 2.0), (326, 2.0), (327, 2.0), (328, 2.0), (329, 2.0), (330, 14.0), (331, 3.0), (332, 2.0), (333, 4.0), (334, 10.0), (335, 3.0), (336, 2.0), (337, 4.0), (338, 3.0), (339, 2.0), (340, 2.0), (341, 5.0), (342, 2.0), (343, 2.0), (344, 2.0), (345, 2.0), (346, 8.0), (347, 3.0), (348, 5.0), (349, 2.0), (350, 3.0), (351, 4.0), (352, 3.0), (353, 13.0), (354, 3.0), (355, 2.0), (356, 2.0), (357, 2.0), (358, 2.0), (359, 7.0), (360, 3.0), (361, 3.0), (362, 2.0), (363, 6.0), (364, 5.0), (365, 2.0), (366, 2.0), (367, 2.0), (368, 2.0), (369, 4.0), (370, 2.0), (371, 2.0), (372, 4.0), (373, 3.0), (374, 3.0), (375, 2.0), (376, 2.0), (377, 8.0), (378, 2.0), (379, 18.0), (380, 2.0), (381, 2.0), (382, 2.0), (383, 2.0), (384, 4.0), (385, 3.0), (386, 3.0), (387, 6.0), (388, 3.0), (389, 2.0), (390, 2.0), (391, 8.0), (392, 4.0), (393, 8.0), (394, 13.0), (395, 4.0), (396, 13.0), (397, 5.0), (398, 3.0), (399, 2.0), (400, 4.0), (401, 2.0), (402, 3.0), (403, 8.0), (404, 3.0), (405, 2.0), (406, 2.0), (407, 13.0), (408, 8.0), (409, 8.0), (410, 9.0), (411, 2.0), (412, 4.0), (413, 2.0), (414, 3.0), (415, 3.0), (416, 2.0), (417, 4.0), (418, 3.0), (419, 3.0), (420, 2.0), (421, 3.0), (422, 2.0), (423, 2.0), (424, 3.0), (425, 3.0), (426, 5.0), (427, 2.0), (428, 2.0), (429, 3.0), (430, 10.0), (431, 2.0), (432, 5.0), (433, 2.0), (434, 3.0), (435, 2.0), (436, 11.0), (437, 2.0), (438, 3.0), (439, 3.0), (440, 2.0), (441, 2.0), (442, 5.0), (443, 3.0), (444, 2.0), (445, 2.0), (446, 2.0), (447, 2.0), (448, 3.0), (449, 4.0), (450, 2.0), (451, 2.0), (452, 2.0), (453, 2.0), (454, 4.0), (455, 2.0), (456, 2.0), (457, 3.0), (458, 5.0), (459, 2.0), (460, 2.0), (461, 2.0), (462, 2.0), (463, 3.0), (464, 2.0), (465, 2.0), (466, 2.0), (467, 3.0), (468, 2.0), (469, 2.0), (470, 11.0), (471, 3.0), (472, 4.0), (473, 2.0), (474, 2.0), (475, 3.0), (476, 2.0), (477, 2.0), (478, 2.0), (479, 4.0), (480, 9.0), (481, 6.0), (482, 2.0), (483, 2.0), (484, 4.0), (485, 2.0), (486, 2.0), (487, 3.0), (488, 2.0), (489, 2.0), (490, 2.0), (491, 5.0), (492, 2.0), (493, 2.0), (494, 2.0), (495, 4.0), (496, 5.0), (497, 2.0), (498, 6.0), (499, 2.0), (500, 2.0), (501, 2.0), (502, 2.0), (503, 10.0), (504, 3.0), (505, 2.0), (506, 4.0), (507, 2.0), (508, 2.0), (509, 2.0), (510, 5.0), (511, 4.0), (512, 2.0), (513, 5.0)]\n",
      "\n",
      " \n",
      "\n",
      "Topic distribution:\n",
      " [(2, 0.12750895080325217), (4, 0.8711027413415442)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(2, 0.91374454448451647), (5, 0.015631785599448892), (8, 0.063400536035754199)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(2, 0.98371097634609694), (5, 0.010099271987725205)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(2, 0.93934367073437008), (5, 0.031254520890112783), (6, 0.013249692590667293), (9, 0.012090699827483545)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(5, 0.18279792010505819), (8, 0.81429232987406908)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(8, 0.99927984246499946)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(8, 0.99898326845330221)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(8, 0.98682048897790753)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(0, 0.81944577259620854), (5, 0.17658663902803573)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(5, 0.9988926670110434)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(9, 0.9947842348804925)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(6, 0.99964234532251539)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(3, 0.99894960270780686)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(1, 0.99952547782712775)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(1, 0.23524573839766552), (2, 0.36389769994087134), (5, 0.31113686995612577), (8, 0.086475681816466102)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(0, 0.99526220707466762)] \n",
      "\n",
      "\n",
      "Topic distribution:\n",
      " [(1, 0.32320234267342712), (2, 0.67539874333363059)] \n",
      "\n",
      "\n",
      "Topic probability (created by corpus.mm and corpus.lda):\n",
      " [[ 0.          0.          0.12750895  0.          0.87110274  0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.91374454  0.          0.          0.01563179\n",
      "   0.          0.          0.06340054  0.        ]\n",
      " [ 0.          0.          0.98371098  0.          0.          0.01009927\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.93934367  0.          0.          0.03125452\n",
      "   0.01324969  0.          0.          0.0120907 ]\n",
      " [ 0.          0.          0.          0.          0.          0.18279792\n",
      "   0.          0.          0.81429233  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.99927984  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.99898327  0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.98682049  0.        ]\n",
      " [ 0.81944577  0.          0.          0.          0.          0.17658664\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.99889267\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.99478423]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.99964235  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.9989496   0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.99952548  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.23524574  0.3638977   0.          0.          0.31113687\n",
      "   0.          0.          0.08647568  0.        ]\n",
      " [ 0.99526221  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.32320234  0.67539874  0.          0.          0.          0.\n",
      "   0.          0.          0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:39 DEBUG collection: Heatmap figure available.\n"
     ]
    }
   ],
   "source": [
    "vis.make_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:40 INFO collection: Saving heatmap figure...\n",
      "29-Nov-2016 12:06:40 DEBUG collection: Heatmap figure available at ./visualizations/heatmap/heatmap.png\n"
     ]
    }
   ],
   "source": [
    "vis.save_heatmap(\"./visualizations/heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:41 INFO collection: Accessing corpus ...\n",
      "29-Nov-2016 12:06:41 INFO gensim.corpora.indexedcorpus: loaded corpus index from out_easy/corpus.mm.index\n",
      "29-Nov-2016 12:06:41 INFO gensim.matutils: initializing corpus reader from out_easy/corpus.mm\n",
      "29-Nov-2016 12:06:41 INFO gensim.matutils: accepted corpus with 17 documents, 514 features, 4585 non-zero entries\n",
      "29-Nov-2016 12:06:41 DEBUG collection: Corpus available.\n",
      "29-Nov-2016 12:06:41 INFO collection: Accessing model ...\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: loading LdaModel object from out_easy/corpus.lda\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: loading id2word recursively from out_easy/corpus.lda.id2word.* with mmap=None\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: setting ignored attribute state to None\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: setting ignored attribute dispatcher to None\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: loading LdaModel object from out_easy/corpus.lda.state\n",
      "29-Nov-2016 12:06:41 DEBUG collection: Model available.\n",
      "29-Nov-2016 12:06:41 DEBUG collection: :param: interactive == True.\n",
      "29-Nov-2016 12:06:41 INFO collection: Accessing dictionary ...\n",
      "29-Nov-2016 12:06:41 INFO gensim.utils: loading Dictionary object from out_easy/corpus.dict\n",
      "29-Nov-2016 12:06:41 DEBUG collection: Dictionary available.\n",
      "29-Nov-2016 12:06:41 DEBUG collection: Corpus, model and dictionary available.\n"
     ]
    }
   ],
   "source": [
    "vis = collection.Visualization(lda_model, corpus, dictionary, doc_labels, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:44 INFO collection: Accessing model, corpus and dictionary ...\n",
      "29-Nov-2016 12:06:44 DEBUG gensim.models.ldamodel: performing inference on a chunk of 17 documents\n",
      "29-Nov-2016 12:06:44 DEBUG gensim.models.ldamodel: 6/17 documents converged within 50 iterations\n",
      "29-Nov-2016 12:06:45 DEBUG collection: Interactive visualization available.\n"
     ]
    }
   ],
   "source": [
    "vis.make_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29-Nov-2016 12:06:45 INFO collection: Saving interactive visualization ...\n",
      "29-Nov-2016 12:06:45 DEBUG collection: Interactive visualization available at ./visualizations/interactive/corpus_interactive.html and ./visualizations/interactive/corpus_interactive.json\n"
     ]
    }
   ],
   "source": [
    "vis.save_interactive(\"./visualizations/interactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success](http://cdn2.hubspot.net/hub/128506/file-446943132-jpg/images/computer_woman_success.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
